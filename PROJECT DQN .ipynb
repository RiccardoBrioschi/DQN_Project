{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6dd34c1",
   "metadata": {},
   "source": [
    "# ü§í Epidemic mitigation project (Riccardo Brioschi, Francesca Venturi)\n",
    "\n",
    "This notebook contains the execution code of the *epidemic mitigation process* carried out by Riccardo Brioschi and Francesca Venturi. \n",
    "\n",
    "Moreover, not only does it contain the code, it also includes comments and discussions about results, coherently with the requirements of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f79cf",
   "metadata": {},
   "source": [
    "## Importing useful packages and Initializing the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee67d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing useful library\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import spaces\n",
    "\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "\"\"\"Environment imports\"\"\"\n",
    "from epidemic_env.env       import Env, Log\n",
    "from epidemic_env.dynamics  import ModelDynamics, Observation\n",
    "from epidemic_env.visualize import Visualize\n",
    "from epidemic_env.agent     import Agent\n",
    "\n",
    "\"\"\"Pytorch and numpy imports\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from os import makedirs\n",
    "from shutil import rmtree\n",
    "\n",
    "seed = 1\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn = ModelDynamics('config/switzerland.yaml')   # load the switzerland map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b92b674",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "We first define the possible actions, encoding them as integers. \n",
    "\n",
    "At this stage of the problem, we can identify 5 different possible actions, as we do not take into account the possibily to combine different decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64153c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_NULL = 0\n",
    "ACTION_CONFINE = 1\n",
    "ACTION_ISOLATE = 2\n",
    "ACTION_HOSPITAL = 3\n",
    "ACTION_VACCINATE = 4\n",
    "\n",
    "\n",
    "def action_preprocessor(a:torch.Tensor, dyn:ModelDynamics):\n",
    "    action = { # DO NOTHING\n",
    "        'confinement': False, \n",
    "        'isolation': False, \n",
    "        'hospital': False, \n",
    "        'vaccinate': False,\n",
    "    }\n",
    "    \n",
    "    if a == ACTION_CONFINE:\n",
    "        action['confinement'] = True\n",
    "    elif a == ACTION_ISOLATE:\n",
    "        action['isolation'] = True\n",
    "    elif a == ACTION_VACCINATE:\n",
    "        action['vaccinate'] = True\n",
    "    elif a == ACTION_HOSPITAL:\n",
    "        action['hospital'] = True\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac24943",
   "metadata": {},
   "source": [
    "### Initializing the environment needed to answer question in part 1. \n",
    "\n",
    "To be coherent with the structure of the tutorial, we decide to encode the actions as integer numbers (that is what is required from the `action_space`) and to exploit the `action_preprocessor` to convert them as a dictionary. Notice that at this stage of the mini-project this is not strictly required, as everything should work equivalently without this change.\n",
    "\n",
    "On the contrary, giving the `observation_reprocessor` as an input argument is essential to later plot and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158654dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env(  dyn, # We pass the dynamical model to the environment \n",
    "            action_space=None, # Here one could pass an openai gym action space that can then be sampled\n",
    "            observation_space=None, # Here one could pass an openai gym obs space that can then be sampled\n",
    "            action_preprocessor=action_preprocessor\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c84584",
   "metadata": {},
   "source": [
    "Since the action we take is always the same (actually, we do not take any action, which means we select `ACTION_NULL`), we initialize it accordingly to the `action_space` and we use it when simulating the epidemic dynamics in our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = ACTION_NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23bbf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []\n",
    "finished = False\n",
    "obs, info = env.reset(seed) # here it is not seeded\n",
    "for t in range(30):\n",
    "    obs, R, finished, info = env.step(action) # always same actions\n",
    "    log.append(info) # save the information dict for logging\n",
    "    if finished:\n",
    "        break\n",
    "\n",
    "\"\"\" Parse the logs \"\"\"\n",
    "total = {p:np.array([getattr(l.total,p) for l in log]) for p in dyn.parameters}\n",
    "cities = {c:{p:np.array([getattr(l.city[c],p) for l in log]) for p in dyn.parameters} for c in dyn.cities}\n",
    "actions = {a:np.array([l.action[a] for l in log]) for a in log[0].action.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b4fa38",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f0ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "[plt.plot(y) for y in total.values()]\n",
    "plt.legend(total.keys())\n",
    "plt.ylabel('number of people in each state')\n",
    "plt.xlabel('time (in weeks)')\n",
    "plt.title('Unmitigated Epidemic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf632da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total['infected'], label = 'infected')\n",
    "plt.plot(total['dead'], label = 'dead')\n",
    "plt.legend()\n",
    "plt.ylabel('number of people in each state')\n",
    "plt.xlabel('time (in weeks)')\n",
    "plt.title('Unmitigated Epidemic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3493232",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,3,figsize=(10,10))\n",
    "row = col = 0\n",
    "for idx, city in enumerate(cities.keys()):\n",
    "    \n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    ax[row,col].plot(cities[city]['infected'],label = 'infected')\n",
    "    ax[row,col].plot(cities[city]['dead'],label = 'dead')\n",
    "    ax[row,col].set_title(city)\n",
    "    \n",
    "# choose title + common legend\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5001cf",
   "metadata": {},
   "source": [
    "### DISCUSSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13382fe",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a99b1d9",
   "metadata": {},
   "source": [
    "### Question 2.a\n",
    "We implement Pr. Russo‚Äôs Policy as a python class (subclass the Agent abstract class provided with the project files) and initialize the agent accordingly.\n",
    "\n",
    "Pr. Russo's Policy consists in confining (`ACTION_CONFINE`) the popoluation for 4 weeks once the infected amount of people exceeds 20000 units. The *confinement action* is not debatable during the confinment. This means that the confinments happen in blocks of (at least) 4 weeks each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e35fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RussoAgent(Agent): # Agent is the superclass\n",
    "    def __init__(self,  env:Env,\n",
    "                # Additionnal parameters to be added here\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Example agent implementation. Just picks a random action at each time step.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.count_remaining_weeks = 0\n",
    "        self.default_action  = ACTION_NULL\n",
    "        self.confinement_action = ACTION_CONFINE\n",
    "        self.confinement_weeks_count = 0\n",
    "        \n",
    "    def load_model(self, savepath):\n",
    "        # This is where one would define the routine for loading a pre-trained model\n",
    "        pass\n",
    "\n",
    "    def save_model(self, savepath):\n",
    "        # This is where one would define the routine for saving the weights for a trained model\n",
    "        pass\n",
    "\n",
    "    def optimize_model(self):\n",
    "        # This is where one would define the optimization step of an RL algorithm\n",
    "        return 0\n",
    "    \n",
    "    def reset(self,env):\n",
    "        # This should be called when the environment is reset (we do not loss any weight, no need to \n",
    "        # redefine actions, the environment is new and we need to save it)\n",
    "        self.env = env\n",
    "        self.count_remaining_weeks = 0\n",
    "        self.confinement_weeks_count = 0\n",
    "    \n",
    "    def act(self, info):\n",
    "        # this takes an observation and returns an action\n",
    "        if self.count_remaining_weeks == 0:\n",
    "            total_infected = info.total.infected # number of infected people at end of week 4\n",
    "            \n",
    "            if total_infected > 20000:\n",
    "                self.count_remaining_weeks = 4\n",
    "                self.confinement_weeks_count += 1\n",
    "                return self.confinement_action\n",
    "            \n",
    "            else:\n",
    "                return self.default_action\n",
    "                \n",
    "        else:\n",
    "            self.count_remaining_weeks -= 1\n",
    "            self.confinement_weeks_count += 1\n",
    "            return self.confinement_action\n",
    "            \n",
    "agent = RussoAgent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1ad4f6",
   "metadata": {},
   "source": [
    "We now run a simulation applying Pr. Russo's Policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d8c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Run the simulation \"\"\"\n",
    "log = []\n",
    "finished = False\n",
    "obs, info = env.reset(seed) # initialization (random infection)\n",
    "agent.reset(env) # useless\n",
    "agent.epsilon = 0 # taken from Agent, which is superclass\n",
    "while not finished:\n",
    "    action = agent.act(info)\n",
    "    obs, R, finished, info = env.step(action)\n",
    "    log.append(info) # save the information dict for logging\n",
    "    if finished:\n",
    "        break\n",
    "\n",
    "total = {p:np.array([getattr(l.total,p) for l in log]) for p in dyn.parameters}\n",
    "cities = {c:{p:np.array([getattr(l.city[c],p) for l in log]) for p in dyn.parameters} for c in dyn.cities}\n",
    "actions = {a:np.array([l.action[a] for l in log]) for a in log[0].action.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a57ec",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d21ca9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[plt.plot(y) for y in total.values()]\n",
    "plt.legend(total.keys())\n",
    "plt.ylabel('number of people in each state')\n",
    "plt.xlabel('time (in weeks)')\n",
    "plt.title('Unmitigated Epidemic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea219a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# andamento a plateau che susssegue la azione che prendiamo, ossia confinement\n",
    "plt.plot(total['infected'], label = 'infected')\n",
    "plt.plot(total['dead'], label = 'dead')\n",
    "plt.legend()\n",
    "plt.ylabel('number of people in each state')\n",
    "plt.xlabel('time (in weeks)')\n",
    "plt.title('Unmitigated Epidemic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a899a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,3,figsize=(10,10))\n",
    "row = col = 0\n",
    "for idx, city in enumerate(cities.keys()):\n",
    "    \n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    ax[row,col].plot(cities[city]['infected'],label = 'infected')\n",
    "    ax[row,col].plot(cities[city]['dead'],label = 'dead')\n",
    "    ax[row,col].set_title(city)\n",
    "    \n",
    "# choose title + common legend\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae41a8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array([v for v in actions.values()]).astype(np.uint8),aspect='auto')\n",
    "plt.title('Actions')\n",
    "plt.yticks([0,1,2,3], labels = list(actions.keys()))\n",
    "plt.xlabel('time (in weeks)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ffdd97",
   "metadata": {},
   "source": [
    "### Question 2.b\n",
    "\n",
    "In order to be able to make meaningful conclusions, we implement the following evaluation procedure: we run 50 simulation episodes where actions are chosen from Pr. Russo's Policy.\n",
    "\n",
    "Notice that, to make results reproducible, we initialize one seed for every episode in the simulation: the i-th simulation corresponds to `seed = i`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d77318",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RussoAgent(env)\n",
    "seeds = range(1,51)\n",
    "\n",
    "# Initializing useful variables\n",
    "conf_days = []\n",
    "rewards = []\n",
    "deaths = []\n",
    "\n",
    "# Looping over 50 episodes\n",
    "for trace in range(50): # for loop over episodes\n",
    "    R_cumulative = 0\n",
    "    finished = False\n",
    "    obs, info = env.reset(seeds[trace]) # resetting the environment\n",
    "    agent.reset(env)\n",
    "    \n",
    "    # Looping over 30 weeks\n",
    "    for t in range(30):\n",
    "        action = agent.act(info)\n",
    "        obs, R, finished, info = env.step(action) \n",
    "        R_cumulative+= R.item()\n",
    "        if finished:\n",
    "            break\n",
    "    \"\"\" Parse the logs \"\"\"\n",
    "    # Saving total number of confined days\n",
    "    conf_days.append(7 * agent.confinement_weeks_count)\n",
    "    # R_cumulative is computed in the inner loop\n",
    "    rewards.append(R_cumulative)\n",
    "    # Number of total deaths in the current episode\n",
    "    deaths.append(info.total.dead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b54ad6",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94376690",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,figsize=(10,7))\n",
    "def hist_avg(ax, data,title):\n",
    "    ymax = 50\n",
    "    if title == 'deaths':\n",
    "        x_range = (1000,200000)\n",
    "    elif title == 'cumulative rewards': \n",
    "        x_range = (-300,300)\n",
    "    elif 'days' in title:\n",
    "        x_range = (0,200)\n",
    "    else:\n",
    "        raise ValueError(f'{title} is not a valid title') \n",
    "    ax.set_title(title)\n",
    "    ax.set_ylim(0,ymax)\n",
    "    ax.vlines([np.mean(data)],0,ymax,color='red')\n",
    "    ax.hist(data,bins=60,range=x_range)\n",
    "hist_avg(ax[0], deaths,'deaths')\n",
    "hist_avg(ax[1], rewards,'cumulative rewards')\n",
    "hist_avg(ax[2], conf_days,'confined days')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\" Print example \"\"\"\n",
    "print(f'Average death number: {np.mean(deaths)}')\n",
    "print(f'Average number of confined days: {np.mean(conf_days)}')\n",
    "print(f'Average cumulative reward: {np.mean(rewards)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b5d50c",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365074b2",
   "metadata": {},
   "source": [
    "### Question 3.a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed192e",
   "metadata": {},
   "source": [
    "We start by defining action preprocessors and observation preprocessor to convert data from environment and neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ef9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Preprocessor: the deafult action (Do Nothing) is encoded as 0, the CONFINEMENT action is encoded as 1\n",
    "\n",
    "ACTION_NULL = 0\n",
    "ACTION_CONFINE = 1\n",
    "\n",
    "def action_preprocessor(a:torch.Tensor, dyn:ModelDynamics):\n",
    "    action = { # DO NOTHING\n",
    "        'confinement': False, \n",
    "        'isolation': False, \n",
    "        'hospital': False, \n",
    "        'vaccinate': False}\n",
    "    \n",
    "    if a == ACTION_CONFINE:\n",
    "        action['confinement'] = True\n",
    "        \n",
    "    return action\n",
    "\n",
    "# Observation Preprocessor: every observation is converted to a tensor containing the proportion of death and\n",
    "# infected people in each city\n",
    "\n",
    "SCALE = 1\n",
    "def observation_preprocessor(obs: Observation, dyn:ModelDynamics):\n",
    "    infected = SCALE * np.array([np.array(obs.city[c].infected)/obs.pop[c] for c in dyn.cities])\n",
    "    dead = SCALE * np.array([np.array(obs.city[c].dead)/obs.pop[c] for c in dyn.cities])\n",
    "    return torch.Tensor(np.stack((infected, dead))).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a213e",
   "metadata": {},
   "source": [
    "We import useful packages, we initialize the environments making sure to define the appropriate observation and action spaces. This is essential, since the action and observation format needed by the dynamic model is different from the encoding taken as input by the neural network we are going to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20548fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the environment that allows us to observe the du√¨ynamics of the pandemic all over Switzerland\n",
    "dyn = ModelDynamics('config/switzerland.yaml')   # load the switzerland map\n",
    "\n",
    "# Initializing the environment\n",
    "env = Env(  dyn, # We pass the dynamical model to the environment \n",
    "            action_space=spaces.Discrete(2) , # Here one could pass an openai gym action space that can then be sampled\n",
    "            observation_space=spaces.Box(low=0, high=1, shape=(2,9,7)),\n",
    "            action_preprocessor=action_preprocessor,\n",
    "            observation_preprocessor=observation_preprocessor)\n",
    "\n",
    "# If gpu is to be used\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d89750",
   "metadata": {},
   "source": [
    "The following cell is standard and we do not have to adapt it to the problem we are solving. This is essential in order to create and define the **Replay Buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ec190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is a named tuple representing a single transition in our environment\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# The following is a cyclic buffer of bounded size that holds the transitions observed recently. \n",
    "# It also implements a .sample() method for selecting a random batch of transitions for training.\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcfd725",
   "metadata": {},
   "source": [
    "We now want to use a neural network to model the Q values for each (state, action) pair. The network architecture follows the suggestion provided in Table 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb72325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        # MLP network\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_observations, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 16),\n",
    "            torch.nn.ReLU(),  \n",
    "            torch.nn.Linear(16, n_actions)\n",
    "        )\n",
    "        \n",
    "    # Called in order to compute the forward pass in the network\n",
    "    def forward(self, x):\n",
    "        new_x = torch.flatten(x,1) ** (1/4) # since we might end up having very small input values due to the scaling\n",
    "                                            # We flatten the input in order to use it in the linear network\n",
    "        z = self.mlp(new_x)\n",
    "        \n",
    "        return z "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c9af6e",
   "metadata": {},
   "source": [
    "We now initiliaze the models we need and other additional useful variables that will be used in the training and evaluation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c38388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "BATCH_SIZE = 2048\n",
    "# GAMMA is the discount factor\n",
    "GAMMA = 0.9\n",
    "# EPS_START is the starting value of epsilon\n",
    "EPS_START = 0.7\n",
    "# EPS_END is the final value of epsilon (in case of decaying epsilon,meant to reduce the exploration)\n",
    "EPS_MIN = 0.2 # same as EPS_START since we do not want to decrease it\n",
    "# DECREASE_FLAG indicates whether we want to decrease the exploration\n",
    "DECREASE_FLAG = False\n",
    "# EXPLORATION_FLAG indicates whether we want to explore or not\n",
    "EXPLORATION_FLAG = True\n",
    "# LR is the learning rate of the AdamW optimizer\n",
    "LR = 5e-3\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Get the number of state observations\n",
    "obs, info = env.reset() # we should set the seed\n",
    "n_observations = 2*9*7\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device) # this is the network we are going to train\n",
    "target_net = DQN(n_observations, n_actions).to(device) # this is the network we are going to keep fixed for some iterates\n",
    "target_net.load_state_dict(policy_net.state_dict())    # we initialize it as a copy of the policy net\n",
    "\n",
    "optimizer = torch.optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True) # we initialize the optimizer\n",
    "memory = ReplayMemory(20000) # we initialize the buffer from which we sample already observed transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ae176",
   "metadata": {},
   "source": [
    "We now initialize a function defining the epsilon greedy policy we use in order to observe transitions that will be later stored in the Replay Buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a617c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function defines the policy deployed in order to explore and interact with the environments.\n",
    "# In particular, it is an epsilon greedy policy based on the values provided by the network trained up to \n",
    "# this moment\n",
    "\n",
    "def select_action(state, decrease_flag, exploration_flag, curr_episode):\n",
    "    sample = random.random()\n",
    "\n",
    "    num_episodes = 500\n",
    "    \n",
    "    # Setting the epsilon parameter to have or avoid exploration\n",
    "    if exploration_flag:\n",
    "        if decrease_flag:\n",
    "            eps_threshold = max(EPS_START*(num_episodes-curr_episode)/num_episodes, EPS_MIN)\n",
    "        else:\n",
    "            eps_threshold = EPS_START\n",
    "    else:\n",
    "        eps_threshold = 0 # in this case we only consider the greedy policy determined by the network\n",
    "        \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # We use torch.no_grad() since we are now using the network to simply take an action.\n",
    "            # We do not need to expand the DAG.\n",
    "            # We only consider the larger value to take the action.\n",
    "            return policy_net(state).max(1)[1].view(1,1) # this format is needed to concatenate\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca2bee3",
   "metadata": {},
   "source": [
    "The following function define the way the policy network is trained. The training happens as follow:\n",
    "- BATCH_SIZE already observed trainsitions are sampled from the replay Buffer in order to train the newtork;\n",
    "\n",
    "- In order to compute the loss (Huber Loss), we define the expected state action_values, which will be considered\n",
    "  as a true label;\n",
    "  \n",
    "- We compute the loss and update the parameters using the optimizer (AdamW) initialized above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfde52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    \n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch. This converts batch-array of Transitions to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements, moving the final results to the device\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    \n",
    "    # We save the non final state in our sampling\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    # We save and concatenate the variables we need\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch)\n",
    "    state_action_values = state_action_values.gather(1, action_batch)\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        # we compute max_a Q(s', a) in order to quantify the delta. We use the target net to improve stability\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = torch.nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04103ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_episodes = 110\n",
    "num_eval_episodes = 20\n",
    "num_weeks = 30\n",
    "training_seeds = range(1, num_episodes + 1)\n",
    "eval_seeds = range(num_episodes + 1, num_episodes + num_eval_episodes + 1)\n",
    "\n",
    "training = []\n",
    "training_weights = []\n",
    "evaluation = []\n",
    "\n",
    "makedirs('./checkpoints_task3', exist_ok=True)\n",
    "rmtree('./checkpoints_task3')\n",
    "makedirs('./checkpoints_task3')\n",
    "PATH = './checkpoints_task3/policy_net'\n",
    "\n",
    "for training_process in range(1, 2):\n",
    "    \n",
    "    print('\\n TRAINING PROCESS :{} \\n'.format(training_process))\n",
    "    \n",
    "    policy_net = DQN(n_observations, n_actions).to(device) # this is the network we are going to train\n",
    "    target_net = DQN(n_observations, n_actions).to(device) # this is the network we are going to keep fixed for some iterates\n",
    "    memory = ReplayMemory(20000) # we initialize the buffer from which we sample already observed transitions\n",
    "    \n",
    "    # Initialize list to keep trace of log rewards\n",
    "    training_trace = []\n",
    "    eval_trace = []\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        if (i_episode % 10) == 0:\n",
    "            print('Training episode :{}'.format(i_episode))\n",
    "\n",
    "        # Initialize variable to keep trace of the total reward\n",
    "        total_training_reward = 0\n",
    "        # Initialize the environment and get its state (moving it to the device)\n",
    "        state, info = env.reset(training_process*training_seeds[i_episode])\n",
    "        state = torch.tensor(state, device=device)\n",
    "\n",
    "        # We run an episode\n",
    "        for t in range(num_weeks):\n",
    "\n",
    "            action = select_action(state, DECREASE_FLAG, EXPLORATION_FLAG, i_episode)\n",
    "            obs, reward, done, info = env.step(action.item())\n",
    "\n",
    "            total_training_reward += reward.item()\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            if done: # in our case it corresponds to 30 weeks\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "            # Store the transition in memory (in the replay buffer)\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # We log the cumulative reward to training trace\n",
    "        training_trace.append(total_training_reward)\n",
    "\n",
    "        # We run a training step on policy_net\n",
    "        optimize_model()\n",
    "\n",
    "        if ((i_episode + 1) % 5 == 0):\n",
    "\n",
    "            # We update the target network every 5 epsiodes\n",
    "            policy_net_state_dict = policy_net.state_dict()\n",
    "            target_net.load_state_dict(policy_net_state_dict)\n",
    "\n",
    "        if ((i_episode + 1) % 50 == 0) or ((i_episode + 1) == 500):\n",
    "\n",
    "            print('Evaluation cycle: {}'.format(int((i_episode + 1) / 50)))\n",
    "            average_rewards = []\n",
    "\n",
    "            for new_episode in range(num_eval_episodes):\n",
    "\n",
    "                total_eval_reward = 0\n",
    "                state, info = env.reset(eval_seeds[new_episode])\n",
    "                state = torch.tensor(state, device=device)\n",
    "\n",
    "                for new_week in range(num_weeks):\n",
    "\n",
    "                    action = select_action(state, DECREASE_FLAG, False, new_episode) # greedy policy when eploration_flag = False\n",
    "                    obs, reward, done, info = env.step(action.item())\n",
    "                    total_eval_reward += reward.item()\n",
    "\n",
    "                    if done: # in our case it corresponds to 30 weeks\n",
    "                        next_state = None\n",
    "                    else:\n",
    "                        next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "                    # Move to the next state\n",
    "                    state = next_state\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                average_rewards.append(total_eval_reward)\n",
    "\n",
    "            eval_trace.append(np.mean(average_rewards))\n",
    "            \n",
    "    torch.save(policy_net.state_dict(), PATH + str(training_process) + '.pt')\n",
    "    training.append(training_trace)\n",
    "    training_weights.append(policy_net_state_dict)\n",
    "    evaluation.append(eval_trace)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9367a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_file = torch.load(PATH + str(training_process) + '.pt')\n",
    "good_mod = DQN(n_observations, n_actions).to(device)\n",
    "good_mod.load_state_dict(good_file)\n",
    "\n",
    "print(good_mod.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e5328a",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced1a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation[0])\n",
    "e = [(evaluation[0][i] + evaluation[1][i] + evaluation[2][i])/3 for i in range(len(eval_trace))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130ed700",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(len(training_trace)), training[0], label='training 1')\n",
    "plt.scatter(np.arange(len(training_trace)), training[1], label='training 2')\n",
    "plt.scatter(np.arange(len(training_trace)), training[2], label='training 3')\n",
    "plt.scatter(np.arange(len(eval_trace)), e, label='evaluation')\n",
    "plt.legend()\n",
    "plt.ylabel('total reward trace')\n",
    "plt.xlabel('episodes')\n",
    "\n",
    "plt.title('Training and Evaluation Traces')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b2fbbf",
   "metadata": {},
   "source": [
    "#### Question 3.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc82951",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0f0117",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70b5913c",
   "metadata": {},
   "source": [
    "Now we can implement a different exploration policy by adjusting some parameters. \n",
    "\n",
    "In particular, we want to implement a decrease in the exploration threshold.\n",
    "This is done to advantage exploration over exploitation in the first stage of the training process, in order to speed up the learning process. The deeper we go in the training, the more information we have about the environment and the transitions, therefore we decrease the exploration coefficient in order to take advantage of exploitation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a78b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPS_START is the starting value of epsilon\n",
    "EPS_START = 0.7\n",
    "# EPS_END is the final value of epsilon (in case of decaying epsilon, meant to reduce the exploration)\n",
    "EPS_MIN = 0.2\n",
    "# DECREASE_FLAG indicates whether we want to decrease the exploration\n",
    "DECREASE_FLAG = True\n",
    "# EXPLORATION_FLAG indicates whether we want to explore or not\n",
    "EXPLORATION_FLAG = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e69d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function defines the policy deployed in order to explore and interact with the environments.\n",
    "# In particular, it is an epsilon greedy policy based on the values provided by the network trained up to \n",
    "# this moment\n",
    "\n",
    "def select_action(state, decrease_flag, exploration_flag, curr_episode):\n",
    "    sample = random.random()\n",
    "    \n",
    "    # Setting the epsilon parameter to have or avoid exploration\n",
    "    if exploration_flag:\n",
    "        if decrease_flag:\n",
    "            eps_threshold = max(EPS_START*(num_episodes-curr_episode)/num_episodes, EPS_MIN)\n",
    "        else:\n",
    "            eps_threshold = EPS_START\n",
    "    else:\n",
    "        eps_threshold = 0 # in this case we only consider the greedy policy determined by the network\n",
    "        \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # We use torch.no_grad() since we are now using the network to simply take an action.\n",
    "            # We do not need to expand the DAG.\n",
    "            # We only consider the larger value to take the action.\n",
    "            return policy_net(state).max(1)[1].view(1,1) # this format is needed to concatenate\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea8654",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 50\n",
    "num_eval_episodes = 20\n",
    "num_weeks = 30\n",
    "training_seeds = range(1,num_episodes + 1)\n",
    "eval_seeds = range(num_episodes + 1, num_episodes + num_eval_episodes + 1)\n",
    "\n",
    "# Initialize list to keep trace of log rewards\n",
    "training_trace = []\n",
    "eval_trace = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    print('Training episode :{}'.format(i_episode))\n",
    "    # Initialize variable to keep trace of the total reward\n",
    "    total_training_reward = 0\n",
    "    # Initialize the environment and get its state (moving it to the device)\n",
    "    state, info = env.reset(training_seeds[i_episode])\n",
    "    state = torch.tensor(state, device=device)\n",
    "    \n",
    "    # We run an episode\n",
    "    for t in range(num_weeks):\n",
    "        \n",
    "        action = select_action(state, DECREASE_FLAG, EXPLORATION_FLAG, i_episode)\n",
    "        obs, reward, done, info = env.step(action.item())\n",
    "        \n",
    "        total_training_reward += reward.item()\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        if done: # in our case it corresponds to 30 weeks\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "        # Store the transition in memory (in the replay buffer)\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # We log the cumulative reward to training trace\n",
    "    training_trace.append(total_training_reward)\n",
    "            \n",
    "    # We run a training step on policy_net\n",
    "    optimize_model()\n",
    "    \n",
    "    if ((i_episode + 1) % 5 == 0):\n",
    "\n",
    "        # We update the target network every 5 epsiodes\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        target_net.load_state_dict(policy_net_state_dict)\n",
    "    \n",
    "    if ((i_episode + 1) % 50 == 0) or ((i_episode +1) == 500):\n",
    "        \n",
    "        average_rewards = []\n",
    "        \n",
    "        for new_episode in range(num_eval_episodes):\n",
    "            \n",
    "            total_eval_reward = 0\n",
    "            state, info = env.reset(eval_seeds[new_episode])\n",
    "            state = torch.tensor(state, device=device)\n",
    "            \n",
    "            for new_week in range(num_weeks):\n",
    "                \n",
    "                action = select_action(state, DECREASE_FLAG, 0, new_episode) # greedy policy when eploration_flag = 0\n",
    "                obs, reward, done, info = env.step(action.item())\n",
    "                total_eval_reward += reward.item()\n",
    "                \n",
    "                if done: # in our case it corresponds to 30 weeks\n",
    "                    next_state = None\n",
    "                else:\n",
    "                    next_state = torch.tensor(obs, device=device)\n",
    "                \n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            average_rewards.append(total_eval_reward)\n",
    "        \n",
    "        eval_trace.append(np.mean(average_rewards))\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24027f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(len(training_trace)), training_trace, label='training')\n",
    "plt.scatter(np.arange(len(eval_trace)), eval_trace, label='training')\n",
    "plt.legend()\n",
    "plt.ylabel('total reward trace')\n",
    "plt.xlabel('episodes')\n",
    "plt.title('Training and Evaluation Traces')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3615e23f",
   "metadata": {},
   "source": [
    "## Question 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8efb95",
   "metadata": {},
   "source": [
    "Setting the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc8ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DECRESE_FLAG = True\n",
    "LR = 10e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeb337c",
   "metadata": {},
   "source": [
    "### Question 4.a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf05a88",
   "metadata": {},
   "source": [
    "con 5 neuroni in pi√π nell'input (embedding dell'azione alla settimana prima) ci fa risparmiare 2^5 neuroni in uscita\n",
    "in uscita abbiamo quindi 5 neuroni invece che 2^5\n",
    "\n",
    "nel caso tabular con (stato agnostico alla situazione attuale sulle azioni), in output dovremmo quindi avere 2^5 neuroni, il che vorrebbe dire che potenzialmente potremmo prendere pi√π 'toggle' contemporaneamente, che non √® possibile\n",
    "\n",
    "dando informazioni sull'azione in input, invece, possiamo ridurre l'output a 5 neuroni e poi applicare una softmax per scegliere quale toggle applicare\n",
    "In questo modo imponiamo che possa essere switchata solo una azione alla volta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Preprocessor: the deafult action (Do Nothing) is encoded as 0, the CONFINEMENT action is encoded as 1\n",
    "\n",
    "TOGGLE_NULL = 0\n",
    "TOGGLE_CONFINEMENT = 1\n",
    "TOGGLE_ISOLATION = 2\n",
    "TOGGLE_HOSPITAL = 3\n",
    "TOGGLE_VACCINATION = 4\n",
    "\n",
    "def action_preprocessor(a:torch.Tensor, dyn:ModelDynamics):\n",
    "    \n",
    "    default_action = { # DO NOTHING\n",
    "        'confinement': False, \n",
    "        'isolation': False, \n",
    "        'hospital': False, \n",
    "        'vaccinate': False}\n",
    "    \n",
    "    if a not in range(0, 5):\n",
    "        print('Not a valid action!')\n",
    "        return default_action\n",
    "    \n",
    "    \n",
    "    action = dyn.get_action()\n",
    "    \n",
    "    if a == TOGGLE_NULL: \n",
    "        return action\n",
    "    elif a == TOGGLE_CONFINEMENT:\n",
    "        action['confinement'] = not action['confinement'] \n",
    "    elif a == TOGGLE_ISOLATION:\n",
    "        action['isolation'] = not action['isolation']\n",
    "    elif a == TOGGLE_HOSPITAL:\n",
    "        action['hospital'] = not action['hospital']\n",
    "    else: \n",
    "        action['vaccinate'] = not action['vaccinate']\n",
    "    \n",
    "    return action\n",
    "\n",
    "# Observation Preprocessor: every observation is converted to a tensor containing the proportion of death and\n",
    "# infected people in each city\n",
    "\n",
    "SCALE = 1\n",
    "def observation_preprocessor(obs: Observation, dyn:ModelDynamics):\n",
    "    infected = SCALE * np.array([np.array(obs.city[c].infected)/obs.pop[c] for c in dyn.cities])\n",
    "    dead = SCALE * np.array([np.array(obs.city[c].dead)/obs.pop[c] for c in dyn.cities])\n",
    "    \n",
    "    curr_action = torch.Tensor(4)\n",
    "    curr_action[0] = 1 if dyn.get_action()['confinement'] else 0\n",
    "    curr_action[1] = 1 if dyn.get_action()['isolation'] else 0\n",
    "    curr_action[2] = 1 if dyn.get_action()['hospital'] else 0\n",
    "    curr_action[3] = 1 if dyn.get_action()['vaccinate'] else 0\n",
    "    \n",
    "    return (torch.Tensor(np.stack((infected, dead))).unsqueeze(0), curr_action.unsqueeze(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the environment that allows us to observe the du√¨ynamics of the pandemic all over Switzerland\n",
    "dyn = ModelDynamics('config/switzerland.yaml')   # load the switzerland map\n",
    "\n",
    "# Initializing the environment\n",
    "env = Env(  dyn, # We pass the dynamical model to the environment \n",
    "            action_space=spaces.Discrete(5) , # Here one could pass an openai gym action space that can then be sampled\n",
    "            # observation_space=spaces.Box(low=0, high=1, shape=(2,9,7)),\n",
    "            action_preprocessor=action_preprocessor,\n",
    "            observation_preprocessor=observation_preprocessor)\n",
    "\n",
    "# If gpu is to be used\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d0c453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is a named tuple representing a single transition in our environment\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'encoded_current_action', 'action', 'next_state','next_encoded_action', 'reward'))\n",
    "\n",
    "# The following is a cyclic buffer of bounded size that holds the transitions observed recently. \n",
    "# It also implements a .sample() method for selecting a random batch of transitions for training.\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde2c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        # MLP network\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_observations + 4, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 16),\n",
    "            torch.nn.ReLU(),  \n",
    "            torch.nn.Linear(16, n_actions)\n",
    "        )\n",
    "        \n",
    "    # Called in order to compute the forward pass in the network\n",
    "    def forward(self, x):\n",
    "        new_x = torch.flatten(x[0],1) ** (1/4) # since we might end up having very small input values due to the scaling\n",
    "                                            # We flatten the input in order to use it in the linear network\n",
    "        \n",
    "        new_y = torch.flatten(x[1],1)\n",
    "        \n",
    "        z = torch.cat((new_x, new_y), dim=1)\n",
    "        \n",
    "        return self.mlp(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f90f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "BATCH_SIZE = 10\n",
    "# GAMMA is the discount factor\n",
    "GAMMA = 0.9\n",
    "# EPS_START is the starting value of epsilon\n",
    "EPS_START = 0.7\n",
    "# EPS_END is the final value of epsilon (in case of decaying epsilon,meant to reduce the exploration)\n",
    "EPS_MIN = 0.2 # same as EPS_START since we do not want to decrease it\n",
    "# DECREASE_FLAG indicates whether we want to decrease the exploration\n",
    "DECREASE_FLAG = False\n",
    "# EXPLORATION_FLAG indicates whether we want to explore or not\n",
    "EXPLORATION_FLAG = True\n",
    "# LR is the learning rate of the AdamW optimizer\n",
    "LR = 5e-3\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Get the number of state observations\n",
    "obs, info = env.reset() # we should set the seed\n",
    "n_observations = 2*9*7\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device) # this is the network we are going to train\n",
    "target_net = DQN(n_observations, n_actions).to(device) # this is the network we are going to keep fixed for some iterates\n",
    "target_net.load_state_dict(policy_net.state_dict())    # we initialize it as a copy of the policy net\n",
    "\n",
    "optimizer = torch.optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True) # we initialize the optimizer\n",
    "memory = ReplayMemory(20000) # we initialize the buffer from which we sample already observed transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdb7b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function defines the policy deployed in order to explore and interact with the environments.\n",
    "# In particular, it is an epsilon greedy policy based on the values provided by the network trained up to \n",
    "# this moment\n",
    "\n",
    "def select_action(state, decrease_flag, exploration_flag, curr_episode):\n",
    "    sample = random.random()\n",
    "    \n",
    "    # Setting the epsilon parameter to have or avoid exploration\n",
    "    if exploration_flag:\n",
    "        if decrease_flag:\n",
    "            eps_threshold = max(EPS_START*(num_episodes-curr_episode)/num_episodes, EPS_MIN)\n",
    "        else:\n",
    "            eps_threshold = EPS_START\n",
    "    else:\n",
    "        eps_threshold = 0 # in this case we only consider the greedy policy determined by the network\n",
    "        \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # We use torch.no_grad() since we are now using the network to simply take an action.\n",
    "            # We do not need to expand the DAG.\n",
    "            # We only consider the larger value to take the action.\n",
    "            return policy_net(state).max(1)[1].view(1,1) # this format is needed to concatenate\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfe6116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    \n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch. This converts batch-array of Transitions to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements, moving the final results to the device\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    \n",
    "    # We save the non final state in our sampling\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    non_final_next_actions = torch.cat([a for a in batch.next_encoded_action if a is not None])\n",
    "    \n",
    "    \n",
    "    # We save and concatenate the variables we need\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    encoded_current_action_batch = torch.cat(batch.encoded_current_action)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net((state_batch, encoded_current_action_batch))\n",
    "    state_action_values = state_action_values.gather(1, action_batch)\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        # we compute max_a Q(s', a) in order to quantify the delta. We use the target net to improve stability\n",
    "        next_state_values[non_final_mask] = target_net((non_final_next_states, non_final_next_actions)).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = torch.nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458755f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 110\n",
    "num_eval_episodes = 20\n",
    "num_weeks = 30\n",
    "training_seeds = range(1, num_episodes + 1)\n",
    "eval_seeds = range(num_episodes + 1, num_episodes + num_eval_episodes + 1)\n",
    "\n",
    "training = []\n",
    "training_weights = []\n",
    "evaluation = []\n",
    "\n",
    "makedirs('./checkpoints_task4', exist_ok=True)\n",
    "rmtree('./checkpoints_task4')\n",
    "makedirs('./checkpoints_task4')\n",
    "PATH = './checkpoints_task4/policy_net'\n",
    "\n",
    "for training_process in range(1, 2):\n",
    "    \n",
    "    print('\\n TRAINING PROCESS :{} \\n'.format(training_process))\n",
    "    \n",
    "    policy_net = DQN(n_observations, n_actions).to(device) # this is the network we are going to train\n",
    "    target_net = DQN(n_observations, n_actions).to(device) # this is the network we are going to keep fixed for some iterates\n",
    "    memory = ReplayMemory(20000) # we initialize the buffer from which we sample already observed transitions\n",
    "    \n",
    "    # Initialize list to keep trace of log rewards\n",
    "    training_trace = []\n",
    "    eval_trace = []\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        if (i_episode % 10) == 0:\n",
    "            print('Training episode :{}'.format(i_episode))\n",
    "\n",
    "        # Initialize variable to keep trace of the total reward\n",
    "        total_training_reward = 0\n",
    "        # Initialize the environment and get its state (moving it to the device)\n",
    "        state, info = env.reset(training_process*training_seeds[i_episode])\n",
    "        \n",
    "        curr_state = torch.tensor(state[0], device=device)\n",
    "        encoded_action = torch.tensor(state[1], device = device)\n",
    "\n",
    "        # We run an episode\n",
    "        for t in range(num_weeks):\n",
    "\n",
    "            action = select_action((curr_state, encoded_action), DECREASE_FLAG, EXPLORATION_FLAG, i_episode)\n",
    "            obs, reward, done, info = env.step(action.item())\n",
    "            \n",
    "            total_training_reward += reward.item()\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            if done: # in our case it corresponds to 30 weeks\n",
    "                next_state = None\n",
    "                next_encoded_action= None\n",
    "            else:\n",
    "                next_state = torch.tensor(obs[0], device=device)\n",
    "                next_encoded_action = torch.tensor(obs[1], device=device)\n",
    "\n",
    "            # Store the transition in memory (in the replay buffer)\n",
    "            memory.push(curr_state, encoded_action, action, next_state, next_encoded_action, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            curr_state = next_state\n",
    "            encoded_action = next_encoded_action\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # We log the cumulative reward to training trace\n",
    "        training_trace.append(total_training_reward)\n",
    "\n",
    "        # We run a training step on policy_net\n",
    "        optimize_model()\n",
    "\n",
    "        if ((i_episode + 1) % 5 == 0):\n",
    "\n",
    "            # We update the target network every 5 epsiodes\n",
    "            policy_net_state_dict = policy_net.state_dict()\n",
    "            target_net.load_state_dict(policy_net_state_dict)\n",
    "\n",
    "        if ((i_episode + 1) % 50 == 0) or ((i_episode + 1) == 500):\n",
    "\n",
    "            print('Evaluation cycle: {}'.format(int((i_episode + 1) / 50)))\n",
    "            average_rewards = []\n",
    "\n",
    "            for new_episode in range(num_eval_episodes):\n",
    "\n",
    "                total_eval_reward = 0\n",
    "                state, info = env.reset(eval_seeds[new_episode])\n",
    "                \n",
    "                curr_state = torch.tensor(state[0], device=device)\n",
    "                encoded_action = torch.tensor(state[1], device = device)\n",
    "\n",
    "                for new_week in range(num_weeks):\n",
    "\n",
    "                    action = select_action((curr_state, encoded_action), DECREASE_FLAG, False, new_episode)\n",
    "                    obs, reward, done, info = env.step(action.item())\n",
    "\n",
    "                    total_training_reward += reward.item()\n",
    "                    reward = torch.tensor([reward], device=device)\n",
    "\n",
    "                    if done: # in our case it corresponds to 30 weeks\n",
    "                        next_state = None\n",
    "                        next_encoded_action= None\n",
    "                    else:\n",
    "                        next_state = torch.tensor(obs[0], device=device)\n",
    "                        next_encoded_action = torch.tensor(obs[1], device=device)\n",
    "\n",
    "                    # Move to the next state\n",
    "                    curr_state = next_state\n",
    "                    encoded_action = next_encoded_action\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                average_rewards.append(total_eval_reward)\n",
    "\n",
    "            eval_trace.append(np.mean(average_rewards))\n",
    "            \n",
    "    torch.save(policy_net.state_dict(), PATH + str(training_process) + '.pt')\n",
    "    training.append(training_trace)\n",
    "    training_weights.append(policy_net_state_dict)\n",
    "    evaluation.append(eval_trace)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25627ac0",
   "metadata": {},
   "source": [
    "#### Question 4.1.d\n",
    "\n",
    "la distanza (in termini di metrica) fra un'azione e la successiva deve essere al pi√π 1\n",
    "ossia possiamo fare il toggling solo di una azione alla volta (al massimo... volendo si pu√≤ anche non cambiare nulla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc178c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_file = torch.load(PATH + str(training_process) + '.pt')\n",
    "good_mod = DQN(n_observations, n_actions).to(device)\n",
    "good_mod.load_state_dict(good_file)\n",
    "\n",
    "print(good_mod.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b68536b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
