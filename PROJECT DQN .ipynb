{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6dd34c1",
   "metadata": {},
   "source": [
    "# ðŸ¤’ Epidemic mitigation project (Riccardo Brioschi, Francesca Venturi)\n",
    "\n",
    "This notebook contains the execution code of the *epidemic mitigation process* carried out by Riccardo Brioschi and Francesca Venturi. \n",
    "\n",
    "Moreover, not only does it contain the code, it also includes comments and discussions about results, coherently with the requirements of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f79cf",
   "metadata": {},
   "source": [
    "## Importing useful packages and Initializing the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee67d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Importing useful library\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import spaces\n",
    "from helper import *\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "\"\"\"Environment imports\"\"\"\n",
    "from epidemic_env.env       import Env, Log\n",
    "from epidemic_env.dynamics  import ModelDynamics, Observation\n",
    "from epidemic_env.visualize import Visualize\n",
    "from epidemic_env.agent     import Agent\n",
    "\n",
    "\"\"\"Pytorch and numpy imports\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\"\"\"Useful libraries to create directories\"\"\"\n",
    "from os import makedirs\n",
    "from shutil import rmtree\n",
    "\n",
    "\"\"\"Ignoring warning to make the code more readable\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\"\"\"If GPU can be used\"\"\"\"\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "\"\"\"Setting the seeds for reproducibility purposes\"\"\"\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn = ModelDynamics('config/switzerland.yaml')   # load the switzerland map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b92b674",
   "metadata": {},
   "source": [
    "## Question 1: Study the behaviour of the model when epidemics are unmitigated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb26f1",
   "metadata": {},
   "source": [
    "We initialize the environment in order to interact with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158654dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env(  dyn, # We pass the dynamical model to the environment \n",
    "            action_space=None, # Here one could pass an openai gym action space that can then be sampled\n",
    "            observation_space=None, # Here one could pass an openai gym obs space that can then be sampled\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c84584",
   "metadata": {},
   "source": [
    "Since the epidemics are unmitigated, in this first task we will always use the null action. Therefore we initialize it in order to call it in the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_NULL = { # DO NOTHING\n",
    "        'confinement': False, \n",
    "        'isolation': False, \n",
    "        'hospital': False, \n",
    "        'vaccinate': False,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e787be49",
   "metadata": {},
   "source": [
    "We now run the epidemic simulation for one episode without epidemic mitigation. We store the results in log to later plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23bbf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []\n",
    "seed = 1\n",
    "finished = False\n",
    "obs, info = env.reset(seed) # here it is not seeded\n",
    "for t in range(30):\n",
    "    obs, R, finished, info = env.step(ACTION_NULL) # always same actions\n",
    "    log.append(info) # save the information dict for logging\n",
    "    if finished:\n",
    "        break\n",
    "\n",
    "\"\"\" Parse the logs \"\"\"\n",
    "total = {p:np.array([getattr(l.total,p) for l in log]) for p in dyn.parameters}\n",
    "cities = {c:{p:np.array([getattr(l.city[c],p) for l in log]) for p in dyn.parameters} for c in dyn.cities}\n",
    "actions = {a:np.array([l.action[a] for l in log]) for a in log[0].action.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b4fa38",
   "metadata": {},
   "source": [
    "We plot the results. Rather then visualize every single result in different cells, we decide to use the code provided by the teaching team in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dafed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_info(total, cities, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5001cf",
   "metadata": {},
   "source": [
    "### DISCUSSION: TO DOOOOOO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13382fe",
   "metadata": {},
   "source": [
    "## Question 2: Professor Russo's Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a99b1d9",
   "metadata": {},
   "source": [
    "### Question 2.a: Implement the policy\n",
    "We implement Pr. Russoâ€™s Policy as a python class (subclass the Agent abstract class provided with the project files) and initialize the agent accordingly. This choice has been made to make the interaction with the initialized environment easier.\n",
    "\n",
    "Pr. Russo's Policy consists in confining (`ACTION_CONFINE`) the population for 4 weeks once the amount of infected people exceeds 20000 units. The *confinement action* is not debatable during the confinment. This means that the confinments happen in blocks of (at least) 4 weeks each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e35fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RussoAgent(Agent): # Agent is the superclass\n",
    "    def __init__(self,  env:Env):\n",
    "        \"\"\"\n",
    "        Initialization of the agent\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        \n",
    "        # Initializing variables to keep trace of weeks to wait to end confinement\n",
    "        self.count_remaining_weeks = 0\n",
    "        \n",
    "        # Initializing default and confinement action, which are returned by the act method depending\n",
    "        # on the amount of infected people\n",
    "        self.default_action  = { # DO NOTHING\n",
    "                                'confinement': False, \n",
    "                                'isolation': False, \n",
    "                                'hospital': False, \n",
    "                                'vaccinate': False,\n",
    "                            }\n",
    "        self.confinement_action = { # CONFINE\n",
    "                                    'confinement': True, \n",
    "                                    'isolation': False, \n",
    "                                    'hospital': False, \n",
    "                                    'vaccinate': False,\n",
    "                                }\n",
    "        \n",
    "        # Initializing variables to keep trace of the number of weeks of confinement\n",
    "        self.confinement_weeks_count = 0\n",
    "        \n",
    "    def load_model(self, savepath):\n",
    "        # This is where one would define the routine for loading a pre-trained model\n",
    "        # This is not initialized since the model does not need to be trained\n",
    "        pass\n",
    "\n",
    "    def save_model(self, savepath):\n",
    "        # This is where one would define the routine for saving the weights for a trained model\n",
    "        # This is not initialized since the model does not need to be trained        \n",
    "        pass\n",
    "\n",
    "    def optimize_model(self):\n",
    "        # This is where one would define the optimization step of an RL algorithm\n",
    "        # This is not initialized since the model does not need to be trained\n",
    "        return 0\n",
    "    \n",
    "    def reset(self, env):\n",
    "        # This should be called when the environment is reset (we do not loss any weight, no need to \n",
    "        # redefine actions, the environment is new and we need to save it)\n",
    "        self.env = env\n",
    "        self.count_remaining_weeks = 0\n",
    "        self.confinement_weeks_count = 0\n",
    "    \n",
    "    def act(self, info):\n",
    "        # This method takes an observation and returns an action\n",
    "        if self.count_remaining_weeks == 0:\n",
    "            total_infected = info.total.infected # number of infected people at end of the week\n",
    "            \n",
    "            if total_infected > 20000:\n",
    "                self.count_remaining_weeks = 3 # since we have just taken the action, there are still 3 weeks to wait\n",
    "                self.confinement_weeks_count += 1\n",
    "                return self.confinement_action\n",
    "            \n",
    "            else:\n",
    "                return self.default_action\n",
    "                \n",
    "        else:\n",
    "            self.count_remaining_weeks -= 1\n",
    "            self.confinement_weeks_count += 1\n",
    "            return self.confinement_action\n",
    "            \n",
    "agent = RussoAgent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1ad4f6",
   "metadata": {},
   "source": [
    "We now run a simulation applying Pr. Russo's Policy to produce the rewuire four plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d8c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Run the simulation \"\"\"\n",
    "log = []\n",
    "finished = False\n",
    "obs, info = env.reset(seed) # initialization (random infection)\n",
    "agent.reset(env) # useless\n",
    "agent.epsilon = 0 # taken from Agent, which is superclass\n",
    "while not finished:\n",
    "    action = agent.act(info)\n",
    "    obs, R, finished, info = env.step(action)\n",
    "    log.append(info) # save the information dict for logging\n",
    "    if finished:\n",
    "        break\n",
    "\n",
    "total = {p:np.array([getattr(l.total,p) for l in log]) for p in dyn.parameters}\n",
    "cities = {c:{p:np.array([getattr(l.city[c],p) for l in log]) for p in dyn.parameters} for c in dyn.cities}\n",
    "actions = {a:np.array([l.action[a] for l in log]) for a in log[0].action.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a57ec",
   "metadata": {},
   "source": [
    "We plot the results. Rather then visualize every single result in different cells, we decide to use the code provided by the teaching team in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d21ca9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_info(total, cities, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ffdd97",
   "metadata": {},
   "source": [
    "### Question 2.b: Evaluate Russo's Policy\n",
    "\n",
    "In order to be able to make meaningful conclusions, we implement the following evaluation procedure: we run 50 simulation episodes where actions are chosen from Pr. Russo's Policy.\n",
    "\n",
    "Notice that, to make results reproducible, we initialize one seed for every episode in the simulation: the i-th simulation corresponds to `seed = i`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d77318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Russo Agent\n",
    "agent = RussoAgent(env)\n",
    "# Initializing the seeds for reproducibility purposes\n",
    "seeds = range(1,51)\n",
    "\n",
    "# Initializing useful variables to store results\n",
    "conf_days = []\n",
    "rewards = []\n",
    "deaths = []\n",
    "\n",
    "# Looping over 50 episodes\n",
    "for trace in range(50): # for loop over episodes\n",
    "    # Initializing provisional variable to save cumulative (non discounted) rewards\n",
    "    R_cumulative = 0\n",
    "    finished = False\n",
    "    # Resetting the environment\n",
    "    obs, info = env.reset(seeds[trace]) \n",
    "    # Synchronizing the agent with the newly defined environment\n",
    "    agent.reset(env)\n",
    "    \n",
    "    # Looping over 30 weeks (lenght of one episode as defined in the pdf)\n",
    "    for t in range(30):\n",
    "        action = agent.act(info)\n",
    "        obs, R, finished, info = env.step(action) \n",
    "        R_cumulative+= R.item()\n",
    "        if finished:\n",
    "            break\n",
    "            \n",
    "    \"\"\" Parse the logs \"\"\"\n",
    "    # Saving total number of confined days\n",
    "    conf_days.append(7 * agent.confinement_weeks_count)\n",
    "    # R_cumulative is computed in the inner loop\n",
    "    rewards.append(R_cumulative)\n",
    "    # Number of total deaths in the current episode\n",
    "    deaths.append(info.total.dead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b54ad6",
   "metadata": {},
   "source": [
    "We plot the histograms of the required quantities, in order to later compare the performance of this model with some more advanced Deep Reinforcement Learning Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94376690",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms(deaths, rewards, conf_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee3023",
   "metadata": {},
   "source": [
    "## Prerequisites and needed data structures for the following tasks (Question 3, 4 and 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf37e96",
   "metadata": {},
   "source": [
    "In the following tasks, we are going to work with Deep Reinforcement Learning Models. In particular, despite some small variations regarding the action and observatin spaces, we are going to implement a DQN model.\n",
    "It is therefore necessary to implement a memory buffer (to store transitions to be used during the off-policy update) and the general structure of the network we are going to use. Notice that all the technical choices, including the choice of hyperparameters, are taken following the suggestions provided by the teaching staff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79d4aea",
   "metadata": {},
   "source": [
    "#### Initializing useful parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f24fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "BATCH_SIZE = 2048\n",
    "# GAMMA is the discount factor\n",
    "GAMMA = 0.9\n",
    "# EPS_START is the starting value of epsilon\n",
    "EPS_START = 0.7\n",
    "# EPS_END is the final value of epsilon (in case of decaying epsilon,meant to reduce the exploration)\n",
    "EPS_MIN = 0.2 # same as EPS_START since we do not want to decrease it\n",
    "# DECREASE_FLAG indicates whether we want to decrease the exploration\n",
    "DECREASE_FLAG = False\n",
    "# EXPLORATION_FLAG indicates whether we want to explore or not\n",
    "EXPLORATION_FLAG = True\n",
    "# LR is the learning rate of the AdamW optimizer\n",
    "LR = 5e-3\n",
    "# Update rate of the target network\n",
    "TAU = 1\n",
    "# Criterion to use in order to compute the loss (Huber Loss)\n",
    "criterion = torch.nn.SmoothL1Loss()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9733bd70",
   "metadata": {},
   "source": [
    "#### Memory Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffebf833",
   "metadata": {},
   "source": [
    "In order to save the transitions obtained from the interactions with the environment, we need to implement a Memory Buffer using a queue. To better deal with the data later, it is useful to assign a precise schema to each element of the buffer. This is achieved by defining the tuple scheme `Transition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898f9e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is a named tuple representing a single transition in our environment\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# The following is a cyclic buffer of bounded size that holds the transitions observed recently. \n",
    "# It also implements a .sample() method for selecting a random batch of transitions for training.\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ce45f",
   "metadata": {},
   "source": [
    "#### Deep Q-Learning Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29531625",
   "metadata": {},
   "source": [
    "We define the network architecture we are going to use in the following tasks. The structure (layers) and the hyperparameters are chosen accordingly to the table provided in the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        # MLP network\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_observations, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 16),\n",
    "            torch.nn.ReLU(),  \n",
    "            torch.nn.Linear(16, n_actions)\n",
    "        )\n",
    "        \n",
    "    # Called in order to compute the forward pass in the network\n",
    "    def forward(self, x):\n",
    "        # We flatten the input in order to use it in the linear network\n",
    "        # Since we might end up having very small input values due to the scaling (see observation preprocessor),\n",
    "        # we compute **(1/4) to increase the magnitude of each term (which is non negative by the dynamics of\n",
    "        # the problem)\n",
    "        new_x = torch.flatten(x,1) ** (1/4) \n",
    "        return self.mlp(new_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da6b54",
   "metadata": {},
   "source": [
    "#### Defining a new Agent: `DQNAgent`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e931ae7",
   "metadata": {},
   "source": [
    "In order to interact with the environment, we define `DQNAgent` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6686bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(Agent): #superclass\n",
    "    def __init__(self, env: Env, Net_Constructor, n_observations, n_actions):\n",
    "        \"\"\"\n",
    "        Example agent implementation. Just picks a random action at each time step.\n",
    "        \"\"\"\n",
    "        # MLP networks. Policy net and target net are used in order to compute the Qvalues\n",
    "        self.policy_net = Net_Constructor(n_observations, n_actions).to(device)\n",
    "        self.target_net = Net_Constructor(n_observations, n_actions).to(device)\n",
    "        # We initialize the target network using the parameters of the policy network\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        # We initialize the optimizer using the suggeseted learning rate (it must act on policy_net parameters)\n",
    "        self.optimizer = torch.optim.AdamW(self.policy_net.parameters(), lr=LR, amsgrad=True) # we initialize the optimizer\n",
    "        self.env = env\n",
    "\n",
    "    def load_model(self, PATH):\n",
    "        \"\"\"Routine for loading a pre-trained model\"\"\"\n",
    "        self.policy_net.load_state_dict(PATH)\n",
    "        pass\n",
    "\n",
    "    def save_model(self, PATH):\n",
    "        \"\"\"Routine for saving the weights for a trained model\"\"\"\n",
    "        torch.save(self.policy_net.state_dict(), PATH + '.pt')\n",
    "\n",
    "    def optimize_model(self, memory, criterion):\n",
    "        \n",
    "        \"\"\"Routine to perform the optimization step updating policy net parameters\"\"\"\n",
    "        \n",
    "        # If there aren't enough data in the memory buffer, avoid computing update step\n",
    "        if len(memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        # Sampling the transitions from the memory buffer\n",
    "        transitions = memory.sample(BATCH_SIZE)\n",
    "        # Transpose the batch. This converts batch-array of Transitions to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements, moving the final results to the device\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "        \n",
    "        # We save the non final state in our sampling\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        \n",
    "        # We save and concatenate the variables we need\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.policy_net(state_batch)\n",
    "        state_action_values = state_action_values.gather(1, action_batch)\n",
    "        \n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            # we compute max_a Q(s', a) in order to quantify the delta. We use the target net to improve stability\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
    "            \n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def reset(self,):\n",
    "        # This should be called when the environment is reset\n",
    "        pass\n",
    "        \n",
    "    def act(self, state, decrease_flag, exploration_flag, curr_episode):\n",
    "        \"\"\" The following function defines the policy deployed in order to explore and interact with the environments.\n",
    "            In particular, it is an epsilon greedy policy (if exploration_flag = True) based on the values provided by \n",
    "            the network trained up to this moment \"\"\"\n",
    "        \n",
    "        # The method takes an observation and returns an action\n",
    "        # In case of random action, the action can be directly sampled using self.env.action_space.sample()\n",
    "        sample = random.random()\n",
    "\n",
    "        # In case of decreasing exploration rate, the following variable is needed\n",
    "        num_episodes = 500\n",
    "        \n",
    "        # Setting the epsilon parameter to have or avoid exploration\n",
    "        if exploration_flag:\n",
    "            if decrease_flag:\n",
    "                eps_threshold = max(EPS_START*(num_episodes-curr_episode)/num_episodes, EPS_MIN)\n",
    "            else:\n",
    "                eps_threshold = EPS_START\n",
    "        else:\n",
    "            eps_threshold = 0 # in this case we only consider the greedy policy determined by the network\n",
    "            \n",
    "        if sample >= eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                # We use torch.no_grad() since we are now using the network to simply take an action.\n",
    "                # We do not need to expand the DAG and consider this computation in the backprop step.\n",
    "                # We only consider the larger value to take the action.\n",
    "                return self.policy_net(state).max(1)[1].view(1,1) # this format is needed to concatenate\n",
    "        else:\n",
    "            return torch.tensor([[self.env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b5d50c",
   "metadata": {},
   "source": [
    "## Question 3: Deep Q-Learning with a binary action space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365074b2",
   "metadata": {},
   "source": [
    "### Question 3.a: Implementing Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed192e",
   "metadata": {},
   "source": [
    "Since the action space is binary and the observation space contains the measurement of the proportion of dead and infected people in each city, each day of a given week, we define action preprocessors and observation preprocessor to convert data from environment and neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4c1e3e",
   "metadata": {},
   "source": [
    "Action Preprocessor: the deafult action (Do Nothing) is encoded as 0, the CONFINEMENT action is encoded as 1.\n",
    "\n",
    "We define it to convert the actions returned by the network to actions recognized by the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ef9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_NULL = 0\n",
    "ACTION_CONFINE = 1\n",
    "\n",
    "def action_preprocessor(a:torch.Tensor, dyn:ModelDynamics):\n",
    "    action = { # DO NOTHING\n",
    "        'confinement': False, \n",
    "        'isolation': False, \n",
    "        'hospital': False, \n",
    "        'vaccinate': False}\n",
    "    \n",
    "    # If a == 1 (the network returns 1), then do confinement\n",
    "    if a == ACTION_CONFINE:\n",
    "        action['confinement'] = True\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c254807",
   "metadata": {},
   "source": [
    "Observation Preprocessor: every observation is converted to a tensor containing the proportion of death and infected people in each city.\n",
    "\n",
    "\n",
    "Since the observation space contains the measurement of the proportion of dead and infected people in each city\n",
    "we naively scale the observation space with SCALE = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = 1\n",
    "def observation_preprocessor(obs: Observation, dyn:ModelDynamics):\n",
    "    infected = SCALE * np.array([np.array(obs.city[c].infected)/obs.pop[c] for c in dyn.cities])\n",
    "    dead = SCALE * np.array([np.array(obs.city[c].dead)/obs.pop[c] for c in dyn.cities])\n",
    "    return torch.Tensor(np.stack((infected, dead))).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a213e",
   "metadata": {},
   "source": [
    "We now initialize the environment making sure to define the appropriate observation and action spaces. This is essential, since the action and observation format needed by the dynamic model is different from the encoding taken as input by the neural network we are going to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20548fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the environment\n",
    "env = Env(  dyn, # We pass the dynamical model to the environment \n",
    "            action_space=spaces.Discrete(2) , # Here one could pass an openai gym action space that can then be sampled\n",
    "            observation_space=spaces.Box(low=0, high=1, shape=(2,9,7)),\n",
    "            action_preprocessor=action_preprocessor,\n",
    "            observation_preprocessor=observation_preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c9af6e",
   "metadata": {},
   "source": [
    "We now initiliaze the models we need and other additional useful variables that will be used in the training and evaluation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04103ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation parameters\n",
    "num_episodes = 500\n",
    "num_eval_episodes = 20\n",
    "num_weeks = 30\n",
    "\n",
    "# Training seeds\n",
    "training_seeds = np.random.randint(22, 10000, size=1500)\n",
    "\n",
    "# Evaluation seeds\n",
    "eval_seeds = np.arange(1, 21)\n",
    "\n",
    "# Initializing empty lists to save results\n",
    "training = []\n",
    "evaluation = []\n",
    "\n",
    "# Initialize the directory to save the model and results of task 3.a\n",
    "makedirs('./checkpoints_task3a', exist_ok=True)\n",
    "rmtree('./checkpoints_task3a')\n",
    "makedirs('./checkpoints_task3a')\n",
    "PATH = './checkpoints_task3a/policy_net'\n",
    "\n",
    "# We run the training and evaluation procedures 3 times, thus obtaining 3 different models. To analyse the\n",
    "# performance of such approach, we are going to consider the best model among the obtained ones\n",
    "\n",
    "for training_process in range(3):\n",
    "    \n",
    "    print('\\n TRAINING PROCESS :{} \\n'.format(training_process+1))\n",
    "\n",
    "    # Ensuring reproducibility, making the net initialization deterministic\n",
    "    torch.manual_seed(training_process)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    \n",
    "    # Initialize the agent for the current training_process\n",
    "    agent = DQNAgent(env, Net_Constructor=DQN, n_observations=2*9*7, n_actions=2)\n",
    "    \n",
    "    # We initialize the buffer from which we sample already observed transitions\n",
    "    memory = ReplayMemory(20000) \n",
    "    \n",
    "    # Initialize list to keep trace of log rewards\n",
    "    training_trace = []\n",
    "    eval_trace = []\n",
    "    \n",
    "    # Looping through the episodes (500 per training process)\n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        if (i_episode % 10) == 0:\n",
    "            print('Training episode :{}'.format(i_episode))\n",
    "\n",
    "        # Initialize variable to keep trace of the total reward\n",
    "        total_training_reward = 0\n",
    "        \n",
    "        # Initialize the environment and get its state (moving it to the device)\n",
    "        state, info = env.reset(training_seeds[500*training_process + i_episode])\n",
    "        state = torch.tensor(state, device=device)\n",
    "\n",
    "        # We run an episode\n",
    "        for t in range(num_weeks):\n",
    "\n",
    "            action = agent.act(state, DECREASE_FLAG, EXPLORATION_FLAG, i_episode)\n",
    "            obs, reward, done, info = env.step(action.item())\n",
    "\n",
    "            total_training_reward += reward.item()\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            if done: # in our case it corresponds to 30 weeks\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "            # Store the transition in memory (in the replay buffer)\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # We log the cumulative reward to training trace\n",
    "        training_trace.append(total_training_reward)\n",
    "\n",
    "        # We run a training step on policy_net\n",
    "        agent.optimize_model(memory, criterion)\n",
    "\n",
    "        if ((i_episode + 1) % 5 == 0):\n",
    "\n",
    "            # We update the target network every 5 epsiodes (using a linear combination of the parameters)\n",
    "            policy_net_state_dict = agent.policy_net.state_dict()\n",
    "            target_net_state_dict = agent.target_net.state_dict()\n",
    "            \n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "\n",
    "            # Changing the parameters of the target network\n",
    "            agent.target_net.load_state_dict(target_net_state_dict)\n",
    "            \n",
    "        # If the following condition is satisfied, we compute an evaluation procedure\n",
    "        if ((i_episode + 1) % 50 == 0) or ((i_episode + 1) == 500):\n",
    "\n",
    "            print('Evaluation cycle: {}'.format(int((i_episode + 1) / 50)))\n",
    "            average_rewards = []\n",
    "\n",
    "            for new_episode in range(num_eval_episodes):\n",
    "\n",
    "                total_eval_reward = 0\n",
    "                state, info = env.reset(eval_seeds[new_episode])\n",
    "                state = torch.tensor(state, device=device)\n",
    "\n",
    "                for new_week in range(num_weeks):\n",
    "\n",
    "                    action = agent.act(state, DECREASE_FLAG, False, new_episode) # greedy policy when eploration_flag = False\n",
    "                    obs, reward, done, info = env.step(action.item())\n",
    "                    total_eval_reward += reward.item()\n",
    "\n",
    "                    if done: # in our case it corresponds to 30 weeks\n",
    "                        next_state = None\n",
    "                    else:\n",
    "                        next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "                    # Move to the next state\n",
    "                    state = next_state\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                average_rewards.append(total_eval_reward)\n",
    "\n",
    "            eval_trace.append(np.mean(average_rewards))\n",
    "    \n",
    "    agent.save_model(PATH + str(training_process))        \n",
    "    training.append(training_trace)\n",
    "    evaluation.append(eval_trace)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01448477",
   "metadata": {},
   "source": [
    "We plot the results obtained during the training and evaluation procedures. Every figure corresponds to a different training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_eval(training, evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ee5f7f",
   "metadata": {},
   "source": [
    "We now proceed to load the best model among the 3. The choice is made depending on the last evaluation value that has been recorded for each process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0497f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select the best model after comparing the last evaluation entries for each training process\n",
    "final_performance = np.array([evaluation[0][-1], evaluation[1][-1], evaluation[2][-1]])\n",
    "best_model_idx = np.argmax(final_performance)\n",
    "\n",
    "# We initialize a new network with the parameters of the best network\n",
    "best_file = torch.load(PATH + str(best_model_idx) + '.pt')\n",
    "best_agent_const = DQNAgent(env, DQN,n_observations=2*9*7, n_actions=2)\n",
    "best_agent_const.policy_net.load_state_dict(best_file)\n",
    "\n",
    "# We create a directory to save the model. This is useful in order not to run the whole training procedure again\n",
    "makedirs('./checkpoints_final', exist_ok=True)\n",
    "rmtree('./checkpoints_final')\n",
    "makedirs('./checkpoints_final')\n",
    "PATH = './checkpoints_final/task3a'\n",
    "\n",
    "# Saving the model parameters\n",
    "torch.save(best_agent_const.policy_net.state_dict(), PATH + '.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b9c5d",
   "metadata": {},
   "source": [
    "We now proceed to simulate 3 episodes using the policy induced by the network. We then plpot the results of one of these episodes to evaluate and discuss the effects of the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40468507",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Run three simulation \"\"\"\n",
    "seeds = np.arange(42, 45)\n",
    "\n",
    "log = []\n",
    "    \n",
    "agent = best_agent_const\n",
    "\n",
    "for i_episode in range(3):\n",
    "\n",
    "    episode_log = []\n",
    "    \n",
    "    # Initialize the environment and get its state (moving it to the device)\n",
    "    state, info = env.reset(seeds[i_episode])\n",
    "    episode_log.append(info)\n",
    "    state = torch.tensor(state, device=device)\n",
    "\n",
    "    # We run an episode\n",
    "    for t in range(num_weeks):\n",
    "\n",
    "        action = agent.act(state, DECREASE_FLAG, False, i_episode)\n",
    "        obs, reward, done, info = env.step(action.item())\n",
    "        episode_log.append(info)\n",
    "\n",
    "        if done: # in our case it corresponds to 30 weeks\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    log.append(episode_log)\n",
    "\n",
    "# computing useful quantities to then plot and visualize the effects of the network\n",
    "total_secondepisode = {p:np.array([getattr(l.total,p) for l in log[1]]) for p in dyn.parameters}\n",
    "cities_secondepisode = {c:{p:np.array([getattr(l.city[c],p) for l in log[1]]) for p in dyn.parameters} for c in dyn.cities}\n",
    "actions_secondepisode = {a:np.array([l.action[a] for l in log[1]]) for a in log[1][0].action.keys()}\n",
    "\n",
    "plot_info(total_secondepisode, cities_secondepisode, actions_secondepisode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d538aa8d",
   "metadata": {},
   "source": [
    "## DISCUSSIONE DA FAREEEE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b2fbbf",
   "metadata": {},
   "source": [
    "### Question 3.b: decreasing exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc82951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECREASE_FLAG indicates whether we want to decrease the exploration\n",
    "DECREASE_FLAG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b5913c",
   "metadata": {},
   "source": [
    "Now we can implement a different exploration policy by adjusting some parameters. \n",
    "\n",
    "In particular, we want to implement a decrease in the exploration threshold.\n",
    "This is done to advantage exploration over exploitation in the first stage of the training process, in order to speed up the learning process. The deeper we go in the training, the more information we have about the environment and the transitions, therefore we decrease the exploration coefficient in order to take advantage of exploitation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c47e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation parameters\n",
    "num_episodes = 500\n",
    "num_eval_episodes = 20\n",
    "num_weeks = 30\n",
    "\n",
    "# Training seeds\n",
    "training_seeds = np.random.randint(22, 10000, size=1500)\n",
    "\n",
    "# Evaluation seeds\n",
    "eval_seeds = np.arange(1, 21)\n",
    "\n",
    "# Initializing empty lists to save results\n",
    "training = []\n",
    "training_weights = []\n",
    "evaluation = []\n",
    "\n",
    "# Initialize the directory to save the model and results of task 3.b\n",
    "makedirs('./checkpoints_task3b', exist_ok=True)\n",
    "rmtree('./checkpoints_task3b')\n",
    "makedirs('./checkpoints_task3b')\n",
    "PATH = './checkpoints_task3b/policy_net'\n",
    "\n",
    "# We run the training and evaluation procedures 3 times, thus obtaining 3 different models. To analyse the\n",
    "# performance of such approach, we are going to consider the best model among the obtained ones\n",
    "for training_process in range(3):\n",
    "    \n",
    "    print('\\n TRAINING PROCESS :{} \\n'.format(training_process+1))\n",
    "    \n",
    "    torch.manual_seed(training_process)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    \n",
    "    agent = DQNAgent(env, Net_Constructor=DQN, n_observations=2*9*7, n_actions=2)\n",
    "    memory = ReplayMemory(20000) # we initialize the buffer from which we sample already observed transitions\n",
    "    \n",
    "    # Initialize list to keep trace of log rewards\n",
    "    training_trace = []\n",
    "    eval_trace = []\n",
    "    \n",
    "    # Looping through the episodes (500 per training process)\n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        if (i_episode % 10) == 0:\n",
    "            print('Training episode :{}'.format(i_episode))\n",
    "\n",
    "        # Initialize variable to keep trace of the total reward\n",
    "        total_training_reward = 0\n",
    "        # Initialize the environment and get its state (moving it to the device)\n",
    "        state, info = env.reset(training_seeds[500*training_process + i_episode])\n",
    "        state = torch.tensor(state, device=device)\n",
    "\n",
    "        # We run an episode\n",
    "        for t in range(num_weeks):\n",
    "\n",
    "            action = agent.act(state, DECREASE_FLAG, EXPLORATION_FLAG, i_episode)\n",
    "            obs, reward, done, info = env.step(action.item())\n",
    "\n",
    "            total_training_reward += reward.item()\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            if done: # in our case it corresponds to 30 weeks\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "            # Store the transition in memory (in the replay buffer)\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # We log the cumulative reward to training trace\n",
    "        training_trace.append(total_training_reward)\n",
    "\n",
    "        # We run a training step on policy_net\n",
    "        agent.optimize_model(memory, criterion)\n",
    "\n",
    "        if ((i_episode + 1) % 5 == 0):\n",
    "\n",
    "            # We update the target network every 5 epsiodes\n",
    "            policy_net_state_dict = agent.policy_net.state_dict()\n",
    "            agent.target_net.load_state_dict(policy_net_state_dict)\n",
    "\n",
    "        if ((i_episode + 1) % 50 == 0) or ((i_episode + 1) == 500):\n",
    "\n",
    "            print('Evaluation cycle: {}'.format(int((i_episode + 1) / 50)))\n",
    "            average_rewards = []\n",
    "\n",
    "            for new_episode in range(num_eval_episodes):\n",
    "\n",
    "                total_eval_reward = 0\n",
    "                state, info = env.reset(eval_seeds[new_episode])\n",
    "                state = torch.tensor(state, device=device)\n",
    "\n",
    "                for new_week in range(num_weeks):\n",
    "\n",
    "                    action = agent.act(state, DECREASE_FLAG, False, new_episode) # greedy policy when eploration_flag = False\n",
    "                    obs, reward, done, info = env.step(action.item())\n",
    "                    total_eval_reward += reward.item()\n",
    "\n",
    "                    if done: # in our case it corresponds to 30 weeks\n",
    "                        next_state = None\n",
    "                    else:\n",
    "                        next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "                    # Move to the next state\n",
    "                    state = next_state\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                average_rewards.append(total_eval_reward)\n",
    "\n",
    "            eval_trace.append(np.mean(average_rewards))\n",
    "    \n",
    "    agent.save_model(PATH + str(training_process))        \n",
    "    training.append(training_trace)\n",
    "    training_weights.append(policy_net_state_dict)\n",
    "    evaluation.append(eval_trace)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63903022",
   "metadata": {},
   "source": [
    "We plot the obtained results to discuss the effects of the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa3c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_eval(training, evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665f3fa9",
   "metadata": {},
   "source": [
    "We now proceed to load the best model among the 3. The choice is made depending on the last evaluation value that has been recorded for each process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35842fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select the best model after comparing the last evaluation entries for each training process\n",
    "final_performance = np.array([evaluation[0][-1], evaluation[1][-1], evaluation[2][-1]])\n",
    "best_model_idx = np.argmax(final_performance)\n",
    "\n",
    "# We initialize a new network with the parameters of the best network\n",
    "best_file = torch.load(PATH + str(best_model_idx) + '.pt')\n",
    "best_agent_decreasing = DQNAgent(env, DQN,n_observations=2*9*7, n_actions=2)\n",
    "best_agent_decreasing.policy_net.load_state_dict(best_file)\n",
    "\n",
    "# We save the model. This is useful in order not to run the whole training procedure again\n",
    "PATH = './checkpoints_final/task3b'\n",
    "torch.save(best_agent_decreasing.policy_net.state_dict(), PATH + '.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7248763",
   "metadata": {},
   "source": [
    "We now proceed to simulate 3 episodes using the policy induced by the network. We then plpot the results of one of these episodes to evaluate and discuss the effects of the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e5875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Run three simulation \"\"\"\n",
    "seeds = np.arange(42, 45)\n",
    "\n",
    "log = []\n",
    "    \n",
    "agent = best_agent_decreasing\n",
    "\n",
    "for i_episode in range(3):\n",
    "\n",
    "    episode_log = []\n",
    "    \n",
    "    # Initialize the environment and get its state (moving it to the device)\n",
    "    state, info = env.reset(seeds[i_episode])\n",
    "    episode_log.append(info)\n",
    "    state = torch.tensor(state, device=device)\n",
    "\n",
    "    # We run an episode\n",
    "    for t in range(num_weeks):\n",
    "\n",
    "        action = agent.act(state, DECREASE_FLAG, False, i_episode)\n",
    "        obs, reward, done, info = env.step(action.item())\n",
    "        episode_log.append(info)\n",
    "\n",
    "        if done: # in our case it corresponds to 30 weeks\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    log.append(episode_log)\n",
    "\n",
    "total_secondepisode = {p:np.array([getattr(l.total,p) for l in log[1]]) for p in dyn.parameters}\n",
    "cities_secondepisode = {c:{p:np.array([getattr(l.city[c],p) for l in log[1]]) for p in dyn.parameters} for c in dyn.cities}\n",
    "actions_secondepisode = {a:np.array([l.action[a] for l in log[1]]) for a in log[1][0].action.keys()}\n",
    "\n",
    "plot_info(total_secondepisode, cities_secondepisode, actions_secondepisode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc35e55",
   "metadata": {},
   "source": [
    "### Question 3.c: Evaluate the best performing policy against Pr. Russoâ€™s policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c733ace",
   "metadata": {},
   "source": [
    "## TO DOOOOOOO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b696909",
   "metadata": {},
   "source": [
    "## Question 4: Dealing with a more complex action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3615e23f",
   "metadata": {},
   "source": [
    "## Question 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8efb95",
   "metadata": {},
   "source": [
    "Setting the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc8ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate\n",
    "LR = 10e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeb337c",
   "metadata": {},
   "source": [
    "### Question 4.a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf05a88",
   "metadata": {},
   "source": [
    "con 5 neuroni in piÃ¹ nell'input (embedding dell'azione alla settimana prima) ci fa risparmiare 2^5 neuroni in uscita\n",
    "in uscita abbiamo quindi 5 neuroni invece che 2^5\n",
    "\n",
    "nel caso tabular con (stato agnostico alla situazione attuale sulle azioni), in output dovremmo quindi avere 2^5 neuroni, il che vorrebbe dire che potenzialmente potremmo prendere piÃ¹ 'toggle' contemporaneamente, che non Ã¨ possibile\n",
    "\n",
    "dando informazioni sull'azione in input, invece, possiamo ridurre l'output a 5 neuroni e poi applicare una softmax per scegliere quale toggle applicare\n",
    "In questo modo imponiamo che possa essere switchata solo una azione alla volta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Preprocessor: the deafult action (Do Nothing) is encoded as 0, the CONFINEMENT action is encoded as 1\n",
    "\n",
    "TOGGLE_NULL = 0\n",
    "TOGGLE_CONFINEMENT = 1\n",
    "TOGGLE_ISOLATION = 2\n",
    "TOGGLE_HOSPITAL = 3\n",
    "TOGGLE_VACCINATION = 4\n",
    "\n",
    "def action_preprocessor(a:torch.Tensor, dyn:ModelDynamics):\n",
    "    \n",
    "    default_action = { # DO NOTHING\n",
    "        'confinement': False, \n",
    "        'isolation': False, \n",
    "        'hospital': False, \n",
    "        'vaccinate': False}\n",
    "    \n",
    "    if a not in range(0, 5):\n",
    "        print('Not a valid action!')\n",
    "        return default_action\n",
    "    \n",
    "    \n",
    "    action = dyn.get_action()\n",
    "    \n",
    "    if a == TOGGLE_NULL: \n",
    "        return action\n",
    "    elif a == TOGGLE_CONFINEMENT:\n",
    "        action['confinement'] = not action['confinement'] \n",
    "    elif a == TOGGLE_ISOLATION:\n",
    "        action['isolation'] = not action['isolation']\n",
    "    elif a == TOGGLE_HOSPITAL:\n",
    "        action['hospital'] = not action['hospital']\n",
    "    else: \n",
    "        action['vaccinate'] = not action['vaccinate']\n",
    "    \n",
    "    return action\n",
    "\n",
    "# Observation Preprocessor: every observation is converted to a tensor containing the proportion of death and\n",
    "# infected people in each city\n",
    "\n",
    "SCALE = 1\n",
    "def observation_preprocessor(obs: Observation, dyn:ModelDynamics):\n",
    "    infected = SCALE * np.array([np.array(obs.city[c].infected)/obs.pop[c] for c in dyn.cities])\n",
    "    dead = SCALE * np.array([np.array(obs.city[c].dead)/obs.pop[c] for c in dyn.cities])\n",
    "    \n",
    "    curr_action = torch.Tensor(4)\n",
    "    curr_action[0] = 1 if dyn.get_action()['confinement'] else 0\n",
    "    curr_action[1] = 1 if dyn.get_action()['isolation'] else 0\n",
    "    curr_action[2] = 1 if dyn.get_action()['hospital'] else 0\n",
    "    curr_action[3] = 1 if dyn.get_action()['vaccinate'] else 0\n",
    "    \n",
    "    ret = torch.flatten(torch.Tensor(np.stack((infected, dead))).unsqueeze(0),1)\n",
    "    ret = torch.cat([ret, curr_action.unsqueeze(dim=0)], dim=1)\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the environment\n",
    "env = Env(  dyn, # We pass the dynamical model to the environment \n",
    "            action_space=spaces.Discrete(5) , # Here one could pass an openai gym action space that can then be sampled\n",
    "            # observation_space=spaces.Box(low=0, high=1, shape=(2,9,7)),\n",
    "            action_preprocessor=action_preprocessor,\n",
    "            observation_preprocessor=observation_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 500\n",
    "num_eval_episodes = 20\n",
    "num_weeks = 30\n",
    "\n",
    "# training seeds\n",
    "training_seeds = np.random.randint(22, 10000, size=1500)\n",
    "\n",
    "# evaluation seeds\n",
    "eval_seeds = np.arange(1, 21)\n",
    "\n",
    "training = []\n",
    "training_weights = []\n",
    "evaluation = []\n",
    "\n",
    "makedirs('./checkpoints_task4a', exist_ok=True)\n",
    "rmtree('./checkpoints_task4a')\n",
    "makedirs('./checkpoints_task4a')\n",
    "PATH = './checkpoints_task4a/policy_net'\n",
    "\n",
    "for training_process in range(3):\n",
    "    \n",
    "    print('\\n TRAINING PROCESS :{} \\n'.format(training_process+1))\n",
    "    \n",
    "    torch.manual_seed(training_process)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    \n",
    "    agent = DQNAgent(env, Net_Constructor=DQN, n_observations=2*9*7, n_actions=2)\n",
    "    memory = ReplayMemory(20000) # we initialize the buffer from which we sample already observed transitions\n",
    "    \n",
    "    # Initialize list to keep trace of log rewards\n",
    "    training_trace = []\n",
    "    eval_trace = []\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        if (i_episode % 10) == 0:\n",
    "            print('Training episode :{}'.format(i_episode))\n",
    "\n",
    "        # Initialize variable to keep trace of the total reward\n",
    "        total_training_reward = 0\n",
    "        # Initialize the environment and get its state (moving it to the device)\n",
    "        state, info = env.reset(training_seeds[500*training_process + i_episode])\n",
    "        state = torch.tensor(state, device=device)\n",
    "\n",
    "        # We run an episode\n",
    "        for t in range(num_weeks):\n",
    "\n",
    "            action = agent.act(state, DECREASE_FLAG, EXPLORATION_FLAG, i_episode)\n",
    "            obs, reward, done, info = env.step(action.item())\n",
    "\n",
    "            total_training_reward += reward.item()\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            if done: # in our case it corresponds to 30 weeks\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "            # Store the transition in memory (in the replay buffer)\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # We log the cumulative reward to training trace\n",
    "        training_trace.append(total_training_reward)\n",
    "\n",
    "        # We run a training step on policy_net\n",
    "        agent.optimize_model(memory, criterion)\n",
    "\n",
    "        if ((i_episode + 1) % 5 == 0):\n",
    "\n",
    "            # We update the target network every 5 epsiodes\n",
    "            policy_net_state_dict = agent.policy_net.state_dict()\n",
    "            agent.target_net.load_state_dict(policy_net_state_dict)\n",
    "\n",
    "        if ((i_episode + 1) % 50 == 0) or ((i_episode + 1) == 500):\n",
    "\n",
    "            print('Evaluation cycle: {}'.format(int((i_episode + 1) / 50)))\n",
    "            average_rewards = []\n",
    "\n",
    "            for new_episode in range(num_eval_episodes):\n",
    "\n",
    "                total_eval_reward = 0\n",
    "                state, info = env.reset(eval_seeds[new_episode])\n",
    "                state = torch.tensor(state, device=device)\n",
    "\n",
    "                for new_week in range(num_weeks):\n",
    "\n",
    "                    action = agent.act(state, DECREASE_FLAG, False, new_episode) # greedy policy when eploration_flag = False\n",
    "                    obs, reward, done, info = env.step(action.item())\n",
    "                    total_eval_reward += reward.item()\n",
    "\n",
    "                    if done: # in our case it corresponds to 30 weeks\n",
    "                        next_state = None\n",
    "                    else:\n",
    "                        next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "                    # Move to the next state\n",
    "                    state = next_state\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                average_rewards.append(total_eval_reward)\n",
    "\n",
    "            eval_trace.append(np.mean(average_rewards))\n",
    "    \n",
    "    agent.save_model(PATH + str(training_process))        \n",
    "    training.append(training_trace)\n",
    "    training_weights.append(policy_net_state_dict)\n",
    "    evaluation.append(eval_trace)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d4101",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_eval(training, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221fd626",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_performance = np.array([evaluation[0][-1], evaluation[1][-1], evaluation[2][-1]])\n",
    "best_model_idx = np.argmax(final_performance)\n",
    "\n",
    "best_file = torch.load(PATH + str(best_model_idx) + '.pt')\n",
    "best_agent_decreasing = DQNAgent(env, DQN,n_observations=2*9*7, n_actions=2)\n",
    "best_agent_decreasing.policy_net.load_state_dict(best_file)\n",
    "\n",
    "PATH = './checkpoints_final/task3b'\n",
    "\n",
    "torch.save(best_agent_decreasing.policy_net.state_dict(), PATH + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f288b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Run three simulation \"\"\"\n",
    "seeds = np.arange(42, 45)\n",
    "\n",
    "log = []\n",
    "    \n",
    "agent = best_agent_decreasing\n",
    "\n",
    "for i_episode in range(3):\n",
    "\n",
    "    episode_log = []\n",
    "    \n",
    "    # Initialize the environment and get its state (moving it to the device)\n",
    "    state, info = env.reset(seeds[i_episode])\n",
    "    episode_log.append(info)\n",
    "    state = torch.tensor(state, device=device)\n",
    "\n",
    "    # We run an episode\n",
    "    for t in range(num_weeks):\n",
    "\n",
    "        action = agent.act(state, DECREASE_FLAG, False, i_episode)\n",
    "        obs, reward, done, info = env.step(action.item())\n",
    "        episode_log.append(info)\n",
    "\n",
    "        if done: # in our case it corresponds to 30 weeks\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    log.append(episode_log)\n",
    "\n",
    "total_secondepisode = {p:np.array([getattr(l.total,p) for l in log[1]]) for p in dyn.parameters}\n",
    "cities_secondepisode = {c:{p:np.array([getattr(l.city[c],p) for l in log[1]]) for p in dyn.parameters} for c in dyn.cities}\n",
    "actions_secondepisode = {a:np.array([l.action[a] for l in log[1]]) for a in log[1][0].action.keys()}\n",
    "\n",
    "plot_info(total_secondepisode, cities_secondepisode, actions_secondepisode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25627ac0",
   "metadata": {},
   "source": [
    "#### Question 4.1.d\n",
    "\n",
    "la distanza (in termini di metrica) fra un'azione e la successiva deve essere al piÃ¹ 1\n",
    "ossia possiamo fare il toggling solo di una azione alla volta (al massimo... volendo si puÃ² anche non cambiare nulla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b68536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_preprocessor(a:torch.Tensor, dyn:ModelDynamics):\n",
    "    action = { # DO NOTHING\n",
    "        'confinement': False, \n",
    "        'isolation': False, \n",
    "        'hospital': False, \n",
    "        'vaccinate': False}\n",
    "    \n",
    "    action['confinement'] = bool(a[0])\n",
    "    action['isolation'] = bool(a[1])\n",
    "    action['hospital'] = bool(a[2])\n",
    "    action['vaccinate'] = bool(a[3])\n",
    "    \n",
    "    return action\n",
    "\n",
    "# Observation Preprocessor: every observation is converted to a tensor containing the proportion of death and\n",
    "# infected people in each city\n",
    "\n",
    "SCALE = 1\n",
    "def observation_preprocessor(obs: Observation, dyn:ModelDynamics):\n",
    "    infected = SCALE * np.array([np.array(obs.city[c].infected)/obs.pop[c] for c in dyn.cities])\n",
    "    dead = SCALE * np.array([np.array(obs.city[c].dead)/obs.pop[c] for c in dyn.cities])\n",
    "    return torch.Tensor(np.stack((infected, dead))).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c2d98",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading the environment that allows us to observe the duÃ¬ynamics of the pandemic all over Switzerland\n",
    "dyn = ModelDynamics('config/switzerland.yaml')   # load the switzerland map\n",
    "\n",
    "# Initializing the environment\n",
    "env = Env(  dyn, # We pass the dynamical model to the environment \n",
    "            action_space=spaces.MultiBinary([4,2]) , # Here one could pass an openai gym action space that can then be sampled\n",
    "            observation_space=spaces.Box(low=0, high=1, shape=(2,9,7)),\n",
    "            action_preprocessor=action_preprocessor,\n",
    "            observation_preprocessor=observation_preprocessor)\n",
    "\n",
    "# If gpu is to beused\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1176b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is a named tuple representing a single transition in our environment\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# The following is a cyclic buffer of bounded size that holds the transitions observed recently. \n",
    "# It also implements a .sample() method for selecting a random batch of transitions for training.\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7cf761",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        # MLP network\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_observations, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 16),\n",
    "            torch.nn.ReLU(),  \n",
    "            torch.nn.Linear(16, n_actions)\n",
    "        )\n",
    "        \n",
    "    # Called in order to compute the forward pass in the network\n",
    "    def forward(self, x):\n",
    "        new_x = torch.flatten(x,1) ** (1/4) # since we might end up having very small input values due to the scaling\n",
    "                                            # We flatten the input in order to use it in the linear network\n",
    "        z = self.mlp(new_x)\n",
    "        \n",
    "        return torch.reshape(z, (-1,4,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db44cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "BATCH_SIZE = 10\n",
    "# GAMMA is the discount factor\n",
    "GAMMA = 0.9\n",
    "# EPS_START is the starting value of epsilon\n",
    "EPS_START = 0.7\n",
    "# EPS_END is the final value of epsilon (in case of decaying epsilon,meant to reduce the exploration)\n",
    "EPS_MIN = 0.2 # same as EPS_START since we do not want to decrease it\n",
    "# DECREASE_FLAG indicates whether we want to decrease the exploration\n",
    "DECREASE_FLAG = False\n",
    "# EXPLORATION_FLAG indicates whether we want to explore or not\n",
    "EXPLORATION_FLAG = True\n",
    "# LR is the learning rate of the AdamW optimizer\n",
    "LR = 5e-3\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n[0] * env.action_space.n[1]\n",
    "\n",
    "# Get the number of state observations\n",
    "obs, info = env.reset() # we should set the seed\n",
    "n_observations = 2*7*9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dddaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function defines the policy deployed in order to explore and interact with the environments.\n",
    "# In particular, it is an epsilon greedy policy based on the values provided by the network trained up to \n",
    "# this moment\n",
    "\n",
    "def select_action(state, decrease_flag, exploration_flag, curr_episode):\n",
    "    sample = random.random()\n",
    "    \n",
    "    # Setting the epsilon parameter to have or avoid exploration\n",
    "    if exploration_flag:\n",
    "        if decrease_flag:\n",
    "            eps_threshold = max(EPS_START*(num_episodes-curr_episode)/num_episodes, EPS_MIN)\n",
    "        else:\n",
    "            eps_threshold = EPS_START\n",
    "    else:\n",
    "        eps_threshold = 0 # in this case we only consider the greedy policy determined by the network\n",
    "        \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # We use torch.no_grad() since we are now using the network to simply take an action.\n",
    "            # We do not need to expand the DAG.\n",
    "            # We only consider the larger value to take the action.\n",
    "            return policy_net(state).max(2)[1]\n",
    "            \n",
    "    else:\n",
    "        return torch.tensor([env.action_space.sample()], device=device, dtype=torch.long).max(2)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e355f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "state1 = env.observation_space.sample()\n",
    "state1 = torch.tensor(state1, device=device).unsqueeze(0)\n",
    "state2 = env.observation_space.sample()\n",
    "state2 = torch.tensor(state2, device=device).unsqueeze(0)\n",
    "obs = torch.cat([state1, state2], dim=0)\n",
    "policy_net = DQN(n_observations, n_actions).to(device) # this is the network we are going to train\n",
    "print(policy_net(obs))\n",
    "print(policy_net(obs).max(2)[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d15d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    \n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch. This converts batch-array of Transitions to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements, moving the final results to the device\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    \n",
    "    # We save the non final state in our sampling\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    # We save and concatenate the variables we need\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch)\n",
    "    state_action_values = torch.sum(state_action_values.gather(2, action_batch.unsqueeze(2)), dim=1)\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        # we compute max_a Q(s', a) in order to quantify the delta. We use the target net to improve stability\n",
    "        next_state_values[non_final_mask] = torch.sum(target_net(non_final_next_states).max(2)[0], dim=1)\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = torch.nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95ccbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((2,4,2))\n",
    "a.gather(2, target_net(non_final_next_states), (2,4,1)))\n",
    "print(a, a.shape)\n",
    "print(a.gather(2, torch.reshape(torch.tensor([0,0,1,0,0,0,0,0]), (2,4,1))))\n",
    "print(torch.sum(a.gather(2, target_net(non_final_next_states), (2,4,1))), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 110\n",
    "num_eval_episodes = 20\n",
    "num_weeks = 30\n",
    "training_seeds = range(1, num_episodes + 1)\n",
    "eval_seeds = range(num_episodes + 1, num_episodes + num_eval_episodes + 1)\n",
    "\n",
    "training = []\n",
    "training_weights = []\n",
    "evaluation = []\n",
    "\n",
    "makedirs('./checkpoints_task4', exist_ok=True)\n",
    "rmtree('./checkpoints_task4')\n",
    "makedirs('./checkpoints_task4')\n",
    "PATH = './checkpoints_task4/policy_net'\n",
    "\n",
    "for training_process in range(1, 2):\n",
    "    \n",
    "    print('\\n TRAINING PROCESS :{} \\n'.format(training_process))\n",
    "    \n",
    "    policy_net = DQN(n_observations, n_actions).to(device) # this is the network we are going to train\n",
    "    target_net = DQN(n_observations, n_actions).to(device) # this is the network we are going to keep fixed for some iterates\n",
    "    target_net.load_state_dict(policy_net.state_dict())    # we initialize it as a copy of the policy net\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True) # we initialize the optimizer\n",
    "    \n",
    "    memory = ReplayMemory(20000) # we initialize the buffer from which we sample already observed transitions\n",
    "    \n",
    "    # Initialize list to keep trace of log rewards\n",
    "    training_trace = []\n",
    "    eval_trace = []\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        if (i_episode % 10) == 0:\n",
    "            print('Training episode :{}'.format(i_episode))\n",
    "\n",
    "        # Initialize variable to keep trace of the total reward\n",
    "        total_training_reward = 0\n",
    "        # Initialize the environment and get its state (moving it to the device)\n",
    "        state, info = env.reset(training_seeds[500*(training_process-1) + i_episode])\n",
    "        \n",
    "        state = torch.tensor(state, device=device)\n",
    "        \n",
    "        # We run an episode\n",
    "        for t in range(num_weeks):\n",
    "\n",
    "            action = select_action(state, DECREASE_FLAG, EXPLORATION_FLAG, i_episode)\n",
    "            obs, reward, done, info = env.step(action.squeeze())\n",
    "\n",
    "            total_training_reward += reward.item()\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            if done: # in our case it corresponds to 30 weeks\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(obs, device=device)\n",
    "                \n",
    "            # Store the transition in memory (in the replay buffer)\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # We log the cumulative reward to training trace\n",
    "        training_trace.append(total_training_reward)\n",
    "\n",
    "        # We run a training step on policy_net\n",
    "        optimize_model()\n",
    "\n",
    "        if ((i_episode + 1) % 5 == 0):\n",
    "\n",
    "            # We update the target network every 5 epsiodes\n",
    "            policy_net_state_dict = policy_net.state_dict()\n",
    "            target_net.load_state_dict(policy_net_state_dict)\n",
    "\n",
    "        if ((i_episode + 1) % 50 == 0) or ((i_episode + 1) == 500):\n",
    "\n",
    "            print('Evaluation cycle: {}'.format(int((i_episode + 1) / 50)))\n",
    "            average_rewards = []\n",
    "\n",
    "            for new_episode in range(num_eval_episodes):\n",
    "\n",
    "                total_eval_reward = 0\n",
    "                state, info = env.reset(eval_seeds[new_episode])\n",
    "                state = torch.tensor(state, device=device)\n",
    "                \n",
    "                for new_week in range(num_weeks):\n",
    "\n",
    "                    action = select_action(state, DECREASE_FLAG, False, new_episode)\n",
    "                    obs, reward, done, info = env.step(action)\n",
    "\n",
    "                    total_training_reward += reward.item()\n",
    "                    reward = torch.tensor([reward], device=device)\n",
    "\n",
    "                    if done: # in our case it corresponds to 30 weeks\n",
    "                        next_state = None\n",
    "                    else:\n",
    "                        next_state = torch.tensor(obs, device=device)\n",
    "                        \n",
    "                    # Move to the next state\n",
    "                    state = next_state\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                average_rewards.append(total_eval_reward)\n",
    "\n",
    "            eval_trace.append(np.mean(average_rewards))\n",
    "            \n",
    "    torch.save(policy_net.state_dict(), PATH + str(training_process) + '.pt')\n",
    "    training.append(training_trace)\n",
    "    training_weights.append(policy_net_state_dict)\n",
    "    evaluation.append(eval_trace)\n",
    "\n",
    "print('Complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
