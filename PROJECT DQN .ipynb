{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6dd34c1",
   "metadata": {},
   "source": [
    "# ðŸ¤’ Epidemic mitigation project (Riccardo Brioschi, Francesca Venturi)\n",
    "\n",
    "This notebook contains the execution code of the *epidemic mitigation process* carried out by Riccardo Brioschi and Francesca Venturi. \n",
    "\n",
    "Moreover, not only does it contain the code, it also includes comments and discussions about results, coherently with the requirements of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f79cf",
   "metadata": {},
   "source": [
    "## Importing useful packages and Initializing the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ee67d5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'line_profiler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mload_ext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mautoreload\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mautoreload\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_line_magic(\u001b[39m'\u001b[39;49m\u001b[39mload_ext\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mline_profiler\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2414\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2412\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mlocal_ns\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2413\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2414\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2416\u001b[0m \u001b[39m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2417\u001b[0m \u001b[39m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2418\u001b[0m \u001b[39m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2419\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(fn, magic\u001b[39m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[39mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/IPython/core/magics/extension.py:33\u001b[0m, in \u001b[0;36mExtensionMagics.load_ext\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m module_str:\n\u001b[1;32m     32\u001b[0m     \u001b[39mraise\u001b[39;00m UsageError(\u001b[39m'\u001b[39m\u001b[39mMissing module name.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshell\u001b[39m.\u001b[39;49mextension_manager\u001b[39m.\u001b[39;49mload_extension(module_str)\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39malready loaded\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     36\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m extension is already loaded. To reload it, use:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m module_str)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/IPython/core/extensions.py:76\u001b[0m, in \u001b[0;36mExtensionManager.load_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load an IPython extension by its module name.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[39mReturns the string \"already loaded\" if the extension is already loaded,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39m\"no load function\" if the module doesn't have a load_ipython_extension\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39mfunction, or None if it succeeded.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_extension(module_str)\n\u001b[1;32m     77\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[39mif\u001b[39;00m module_str \u001b[39min\u001b[39;00m BUILTINS_EXTS:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/IPython/core/extensions.py:91\u001b[0m, in \u001b[0;36mExtensionManager._load_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshell\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[1;32m     90\u001b[0m     \u001b[39mif\u001b[39;00m module_str \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules:\n\u001b[0;32m---> 91\u001b[0m         mod \u001b[39m=\u001b[39m import_module(module_str)\n\u001b[1;32m     92\u001b[0m     mod \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mmodules[module_str]\n\u001b[1;32m     93\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_load_ipython_extension(mod):\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:984\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'line_profiler'"
     ]
    }
   ],
   "source": [
    "\"\"\"Importing useful library\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import spaces\n",
    "from helper import *\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"Environment imports\"\"\"\n",
    "from epidemic_env.env       import Env, Log\n",
    "from epidemic_env.dynamics  import ModelDynamics, Observation\n",
    "from epidemic_env.visualize import Visualize\n",
    "from epidemic_env.agent     import Agent\n",
    "\n",
    "\"\"\"Pytorch and numpy imports\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\"\"\"Useful libraries to create directories\"\"\"\n",
    "from os import makedirs\n",
    "from shutil import rmtree\n",
    "\n",
    "\"\"\"Ignoring warning to make the code more readable\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\"\"\"If GPU can be used\"\"\"\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "\"\"\"Setting the seeds for reproducibility purposes\"\"\"\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn = ModelDynamics('config/switzerland.yaml')   # load the switzerland map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b92b674",
   "metadata": {},
   "source": [
    "## Question 1: Study the behaviour of the model when epidemics are unmitigated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb26f1",
   "metadata": {},
   "source": [
    "We initialize the environment in order to interact with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158654dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env(  dyn, # We pass the dynamical model to the environment \n",
    "            action_space=None, # Here one could pass an openai gym action space that can then be sampled\n",
    "            observation_space=None, # Here one could pass an openai gym obs space that can then be sampled\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c84584",
   "metadata": {},
   "source": [
    "Since the epidemics are unmitigated, in this first task we will always use the null action. Therefore we initialize it in order to call it in the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_NULL = { # DO NOTHING\n",
    "        'confinement': False, \n",
    "        'isolation': False, \n",
    "        'hospital': False, \n",
    "        'vaccinate': False,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e787be49",
   "metadata": {},
   "source": [
    "We now run the epidemic simulation for one episode without epidemic mitigation. We store the results in log to later plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23bbf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []\n",
    "seed = 1\n",
    "finished = False\n",
    "obs, info = env.reset(seed) # here it is not seeded\n",
    "for t in range(30):\n",
    "    obs, R, finished, info = env.step(ACTION_NULL) # always same actions\n",
    "    log.append(info) # save the information dict for logging\n",
    "    if finished:\n",
    "        break\n",
    "\n",
    "\"\"\" Parse the logs \"\"\"\n",
    "total = {p:np.array([getattr(l.total,p) for l in log]) for p in dyn.parameters}\n",
    "cities = {c:{p:np.array([getattr(l.city[c],p) for l in log]) for p in dyn.parameters} for c in dyn.cities}\n",
    "actions = {a:np.array([l.action[a] for l in log]) for a in log[0].action.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b4fa38",
   "metadata": {},
   "source": [
    "We plot the results. Rather then visualize every single result in different cells, we decide to use the code provided by the teaching team in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dafed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_info(total, cities, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5001cf",
   "metadata": {},
   "source": [
    "### DISCUSSION: TO DOOOOOO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13382fe",
   "metadata": {},
   "source": [
    "## Question 2: Professor Russo's Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a99b1d9",
   "metadata": {},
   "source": [
    "### Question 2.a: Implement the policy\n",
    "We implement Pr. Russoâ€™s Policy as a python class (subclass the Agent abstract class provided with the project files) and initialize the agent accordingly. This choice has been made to make the interaction with the initialized environment easier.\n",
    "\n",
    "Pr. Russo's Policy consists in confining (`ACTION_CONFINE`) the population for 4 weeks once the amount of infected people exceeds 20000 units. The *confinement action* is not debatable during the confinment. This means that the confinments happen in blocks of (at least) 4 weeks each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e35fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RussoAgent(Agent): # Agent is the superclass\n",
    "    def __init__(self,  env:Env):\n",
    "        \"\"\"\n",
    "        Initialization of the agent\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        \n",
    "        # Initializing variables to keep trace of weeks to wait to end confinement\n",
    "        self.count_remaining_weeks = 0\n",
    "        \n",
    "        # Initializing default and confinement action, which are returned by the act method depending\n",
    "        # on the amount of infected people\n",
    "        self.default_action  = { # DO NOTHING\n",
    "                                'confinement': False, \n",
    "                                'isolation': False, \n",
    "                                'hospital': False, \n",
    "                                'vaccinate': False,\n",
    "                            }\n",
    "        self.confinement_action = { # CONFINE\n",
    "                                    'confinement': True, \n",
    "                                    'isolation': False, \n",
    "                                    'hospital': False, \n",
    "                                    'vaccinate': False,\n",
    "                                }\n",
    "        \n",
    "        # Initializing variables to keep trace of the number of weeks of confinement\n",
    "        self.confinement_weeks_count = 0\n",
    "        \n",
    "    def load_model(self, savepath):\n",
    "        # This is where one would define the routine for loading a pre-trained model\n",
    "        # This is not initialized since the model does not need to be trained\n",
    "        pass\n",
    "\n",
    "    def save_model(self, savepath):\n",
    "        # This is where one would define the routine for saving the weights for a trained model\n",
    "        # This is not initialized since the model does not need to be trained        \n",
    "        pass\n",
    "\n",
    "    def optimize_model(self):\n",
    "        # This is where one would define the optimization step of an RL algorithm\n",
    "        # This is not initialized since the model does not need to be trained\n",
    "        return 0\n",
    "    \n",
    "    def reset(self, env):\n",
    "        # This should be called when the environment is reset (we do not loss any weight, no need to \n",
    "        # redefine actions, the environment is new and we need to save it)\n",
    "        self.env = env\n",
    "        self.count_remaining_weeks = 0\n",
    "        self.confinement_weeks_count = 0\n",
    "    \n",
    "    def act(self, info):\n",
    "        # This method takes an observation and returns an action\n",
    "        if self.count_remaining_weeks == 0:\n",
    "            total_infected = info.total.infected # number of infected people at end of the week\n",
    "            \n",
    "            if total_infected > 20000:\n",
    "                self.count_remaining_weeks = 3 # since we have just taken the action, there are still 3 weeks to wait\n",
    "                self.confinement_weeks_count += 1\n",
    "                return self.confinement_action\n",
    "            \n",
    "            else:\n",
    "                return self.default_action\n",
    "                \n",
    "        else:\n",
    "            self.count_remaining_weeks -= 1\n",
    "            self.confinement_weeks_count += 1\n",
    "            return self.confinement_action\n",
    "            \n",
    "agent = RussoAgent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1ad4f6",
   "metadata": {},
   "source": [
    "We now run a simulation applying Pr. Russo's Policy to produce the rewuire four plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d8c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Run the simulation \"\"\"\n",
    "log = []\n",
    "finished = False\n",
    "obs, info = env.reset(seed) # initialization (random infection)\n",
    "agent.reset(env) # useless\n",
    "#agent.epsilon = 0 # taken from Agent, which is superclass\n",
    "while not finished:\n",
    "    action = agent.act(info)\n",
    "    obs, R, finished, info = env.step(action)\n",
    "    log.append(info) # save the information dict for logging\n",
    "    if finished:\n",
    "        break\n",
    "\n",
    "total = {p:np.array([getattr(l.total,p) for l in log]) for p in dyn.parameters}\n",
    "cities = {c:{p:np.array([getattr(l.city[c],p) for l in log]) for p in dyn.parameters} for c in dyn.cities}\n",
    "actions = {a:np.array([l.action[a] for l in log]) for a in log[0].action.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a57ec",
   "metadata": {},
   "source": [
    "We plot the results. Rather then visualize every single result in different cells, we decide to use the code provided by the teaching team in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d21ca9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_info(total, cities, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ffdd97",
   "metadata": {},
   "source": [
    "### Question 2.b: Evaluate Russo's Policy\n",
    "\n",
    "In order to be able to make meaningful conclusions, we implement the following evaluation procedure: we run 50 simulation episodes where actions are chosen from Pr. Russo's Policy.\n",
    "\n",
    "Notice that, to make results reproducible, we initialize one seed for every episode in the simulation: the i-th simulation corresponds to `seed = i`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d77318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Russo Agent\n",
    "agent = RussoAgent(env)\n",
    "# Initializing the seeds for reproducibility purposes\n",
    "seeds = range(1,51)\n",
    "\n",
    "# Initializing useful variables to store results\n",
    "conf_days = []\n",
    "rewards = []\n",
    "deaths = []\n",
    "\n",
    "# Looping over 50 episodes\n",
    "for trace in range(50): # for loop over episodes\n",
    "    # Initializing provisional variable to save cumulative (non discounted) rewards\n",
    "    R_cumulative = 0\n",
    "    finished = False\n",
    "    # Resetting the environment\n",
    "    obs, info = env.reset(seeds[trace]) \n",
    "    # Synchronizing the agent with the newly defined environment\n",
    "    agent.reset(env)\n",
    "    \n",
    "    # Looping over 30 weeks (lenght of one episode as defined in the pdf)\n",
    "    for t in range(30):\n",
    "        action = agent.act(info)\n",
    "        obs, R, finished, info = env.step(action) \n",
    "        R_cumulative+= R.item()\n",
    "        if finished:\n",
    "            break\n",
    "            \n",
    "    \"\"\" Parse the logs \"\"\"\n",
    "    # Saving total number of confined days\n",
    "    conf_days.append(7 * agent.confinement_weeks_count)\n",
    "    # R_cumulative is computed in the inner loop\n",
    "    rewards.append(R_cumulative)\n",
    "    # Number of total deaths in the current episode\n",
    "    deaths.append(info.total.dead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b54ad6",
   "metadata": {},
   "source": [
    "We plot the histograms of the required quantities, in order to later compare the performance of this model with some more advanced Deep Reinforcement Learning Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94376690",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms(deaths, rewards, conf_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee3023",
   "metadata": {},
   "source": [
    "## Prerequisites and needed data structures for the following tasks (Question 3 and 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf37e96",
   "metadata": {},
   "source": [
    "In the following tasks, we are going to work with Deep Reinforcement Learning Models. In particular, despite some small variations regarding the action and observatin spaces, we are going to implement a DQN model.\n",
    "It is therefore necessary to implement a memory buffer (to store transitions to be used during the off-policy update) and the general structure of the network we are going to use. Notice that all the technical choices, including the choice of hyperparameters, are taken following the suggestions provided by the teaching staff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79d4aea",
   "metadata": {},
   "source": [
    "#### Initializing useful parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f24fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "BATCH_SIZE = 2048\n",
    "# GAMMA is the discount factor\n",
    "GAMMA = 0.9\n",
    "# EPS_START is the starting value of epsilon\n",
    "EPS_START = 0.7\n",
    "# EPS_END is the final value of epsilon (in case of decaying epsilon,meant to reduce the exploration)\n",
    "EPS_MIN = 0.2 # same as EPS_START since we do not want to decrease it\n",
    "# DECREASE_FLAG indicates whether we want to decrease the exploration\n",
    "DECREASE_FLAG = False\n",
    "# EXPLORATION_FLAG indicates whether we want to explore or not\n",
    "EXPLORATION_FLAG = True\n",
    "# LR is the learning rate of the AdamW optimizer\n",
    "LR = 5e-3\n",
    "# Update rate of the target network\n",
    "TAU = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9733bd70",
   "metadata": {},
   "source": [
    "#### Memory Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffebf833",
   "metadata": {},
   "source": [
    "In order to save the transitions obtained from the interactions with the environment, we need to implement a Memory Buffer using a queue. To better deal with the data later, it is useful to assign a precise schema to each element of the buffer. This is achieved by defining the tuple scheme `Transition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898f9e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is a named tuple representing a single transition in our environment\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# The following is a cyclic buffer of bounded size that holds the transitions observed recently. \n",
    "# It also implements a .sample() method for selecting a random batch of transitions for training.\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ce45f",
   "metadata": {},
   "source": [
    "#### Deep Q-Learning Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29531625",
   "metadata": {},
   "source": [
    "We define the network architecture we are going to use in the following tasks. The structure (layers) and the hyperparameters are chosen accordingly to the table provided in the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        # MLP network\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_observations, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 16),\n",
    "            torch.nn.ReLU(),  \n",
    "            torch.nn.Linear(16, n_actions)\n",
    "        )\n",
    "        \n",
    "    # Called in order to compute the forward pass in the network\n",
    "    def forward(self, x):\n",
    "        # We flatten the input in order to use it in the linear network\n",
    "        # Since we might end up having very small input values due to the scaling (see observation preprocessor),\n",
    "        # we compute **(1/4) to increase the magnitude of each term (which is non negative by the dynamics of\n",
    "        # the problem)\n",
    "        new_x = torch.flatten(x,1) ** (1/4) \n",
    "        return self.mlp(new_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da6b54",
   "metadata": {},
   "source": [
    "#### Defining a new Agent: `DQNAgent`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e931ae7",
   "metadata": {},
   "source": [
    "In order to interact with the environment, we define `DQNAgent` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6686bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(Agent): #superclass\n",
    "    def __init__(self, env: Env, Net_Constructor, n_observations, n_actions):\n",
    "        \"\"\"\n",
    "        Example agent implementation. Just picks a random action at each time step.\n",
    "        \"\"\"\n",
    "        # MLP networks. Policy net and target net are used in order to compute the Qvalues\n",
    "        self.policy_net = Net_Constructor(n_observations, n_actions).to(device)\n",
    "        self.target_net = Net_Constructor(n_observations, n_actions).to(device)\n",
    "        # We initialize the target network using the parameters of the policy network\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        # We initialize the optimizer using the suggeseted learning rate (it must act on policy_net parameters)\n",
    "        self.optimizer = torch.optim.AdamW(self.policy_net.parameters(), lr=LR, amsgrad=True) # we initialize the optimizer\n",
    "        self.env = env\n",
    "\n",
    "        self.memory = ReplayMemory(20000)\n",
    "\n",
    "    def load_model(self, PATH):\n",
    "        \"\"\"Routine for loading a pre-trained model\"\"\"\n",
    "        self.policy_net.load_state_dict(PATH)\n",
    "        pass\n",
    "\n",
    "    def save_model(self, PATH):\n",
    "        \"\"\"Routine for saving the weights for a trained model\"\"\"\n",
    "        torch.save(self.policy_net.state_dict(), PATH + '.pt')\n",
    "\n",
    "    def optimize_model(self):\n",
    "        \n",
    "        \"\"\"Routine to perform the optimization step updating policy net parameters\"\"\"\n",
    "        \n",
    "        # If there aren't enough data in the memory buffer, avoid computing update step\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        # Sampling the transitions from the memory buffer\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        # Transpose the batch. This converts batch-array of Transitions to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements, moving the final results to the device\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "        \n",
    "        # We save the non final state in our sampling\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        \n",
    "        # We save and concatenate the variables we need\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            # we compute max_a Q(s', a) in order to quantify the delta. We use the target net to improve stability\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
    "            \n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = torch.nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def reset(self,):\n",
    "        # This should be called when the environment is reset\n",
    "        pass\n",
    "        \n",
    "    def act(self, state, decrease_flag, exploration_flag, curr_episode):\n",
    "        \"\"\" The following function defines the policy deployed in order to explore and interact with the environments.\n",
    "            In particular, it is an epsilon greedy policy (if exploration_flag = True) based on the values provided by \n",
    "            the network trained up to this moment \"\"\"\n",
    "        \n",
    "        # The method takes an observation and returns an action\n",
    "        # In case of random action, the action can be directly sampled using self.env.action_space.sample()\n",
    "        sample = random.random()\n",
    "\n",
    "        # In case of decreasing exploration rate, the following variable is needed\n",
    "        num_episodes = 500\n",
    "        \n",
    "        # Setting the epsilon parameter to have or avoid exploration\n",
    "        if exploration_flag:\n",
    "            if decrease_flag:\n",
    "                eps_threshold = max(EPS_START*(num_episodes-curr_episode)/num_episodes, EPS_MIN)\n",
    "            else:\n",
    "                eps_threshold = EPS_START\n",
    "        else:\n",
    "            eps_threshold = 0 # in this case we only consider the greedy policy determined by the network\n",
    "            \n",
    "        if sample >= eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                # We use torch.no_grad() since we are now using the network to simply take an action.\n",
    "                # We do not need to expand the DAG and consider this computation in the backprop step.\n",
    "                # We only consider the larger value to take the action.\n",
    "                return self.policy_net(state).max(1)[1].view(1,1) # this format is needed to concatenate\n",
    "        else:\n",
    "            return torch.tensor([[self.env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b5d50c",
   "metadata": {},
   "source": [
    "## Question 3: Deep Q-Learning with a binary action space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365074b2",
   "metadata": {},
   "source": [
    "### Question 3.a: Implementing Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed192e",
   "metadata": {},
   "source": [
    "Since the action space is binary and the observation space contains the measurement of the proportion of dead and infected people in each city, each day of a given week, we define action preprocessors and observation preprocessor to convert data from environment and neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4c1e3e",
   "metadata": {},
   "source": [
    "Action Preprocessor: the deafult action (Do Nothing) is encoded as 0, the CONFINEMENT action is encoded as 1.\n",
    "\n",
    "We define it to convert the actions returned by the network to actions recognized by the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ef9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_NULL = 0\n",
    "ACTION_CONFINE = 1\n",
    "\n",
    "def action_preprocessor(a:torch.Tensor, dyn:ModelDynamics):\n",
    "    action = { # DO NOTHING\n",
    "        'confinement': False, \n",
    "        'isolation': False, \n",
    "        'hospital': False, \n",
    "        'vaccinate': False}\n",
    "    \n",
    "    # If a == 1 (the network returns 1), then do confinement\n",
    "    if a == ACTION_CONFINE:\n",
    "        action['confinement'] = True\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c254807",
   "metadata": {},
   "source": [
    "Observation Preprocessor: every observation is converted to a tensor containing the proportion of death and infected people in each city.\n",
    "\n",
    "\n",
    "Since the observation space contains the measurement of the proportion of dead and infected people in each city\n",
    "we naively scale the observation space with SCALE = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = 1\n",
    "def observation_preprocessor(obs: Observation, dyn:ModelDynamics):\n",
    "    infected = SCALE * np.array([np.array(obs.city[c].infected)/obs.pop[c] for c in dyn.cities])\n",
    "    dead = SCALE * np.array([np.array(obs.city[c].dead)/obs.pop[c] for c in dyn.cities])\n",
    "    return torch.Tensor(np.stack((infected, dead))).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a213e",
   "metadata": {},
   "source": [
    "We now initialize the environment making sure to define the appropriate observation and action spaces. This is essential, since the action and observation format needed by the dynamic model is different from the encoding taken as input by the neural network we are going to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20548fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the environment\n",
    "env = Env(  dyn, # We pass the dynamical model to the environment \n",
    "            action_space=spaces.Discrete(2) , # Here one could pass an openai gym action space that can then be sampled\n",
    "            observation_space=spaces.Box(low=0, high=1, shape=(2,9,7)),\n",
    "            action_preprocessor=action_preprocessor,\n",
    "            observation_preprocessor=observation_preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c9af6e",
   "metadata": {},
   "source": [
    "We now initiliaze the models we need and other additional useful variables that will be used in the training and evaluation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1588a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(env.step(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c04103ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAINING PROCESS :1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 92/500 [02:11<09:43,  1.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m agent\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mpush(state, action, next_state, reward)\n\u001b[1;32m     73\u001b[0m \u001b[39m# We run a training step on policy_net\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m agent\u001b[39m.\u001b[39;49moptimize_model()\n\u001b[1;32m     76\u001b[0m \u001b[39m# Move to the next state\u001b[39;00m\n\u001b[1;32m     77\u001b[0m state \u001b[39m=\u001b[39m next_state\n",
      "Cell \u001b[0;32mIn[17], line 44\u001b[0m, in \u001b[0;36mDQNAgent.optimize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m non_final_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mtuple\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m s: s \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, batch\u001b[39m.\u001b[39mnext_state)), device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mbool)\n\u001b[1;32m     43\u001b[0m \u001b[39m# We save the non final state in our sampling\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m non_final_next_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([s \u001b[39mfor\u001b[39;49;00m s \u001b[39min\u001b[39;49;00m batch\u001b[39m.\u001b[39;49mnext_state \u001b[39mif\u001b[39;49;00m s \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m])\n\u001b[1;32m     46\u001b[0m \u001b[39m# We save and concatenate the variables we need\u001b[39;00m\n\u001b[1;32m     47\u001b[0m state_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(batch\u001b[39m.\u001b[39mstate)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training and evaluation parameters\n",
    "num_episodes = 500\n",
    "num_eval_episodes = 20\n",
    "num_weeks = 30\n",
    "\n",
    "# Training seeds\n",
    "training_seeds = np.random.randint(22, 10000, size=1500)\n",
    "\n",
    "# Evaluation seeds\n",
    "eval_seeds = np.arange(1, 21)\n",
    "\n",
    "# Initializing empty lists to save results\n",
    "training = []\n",
    "evaluation = []\n",
    "\n",
    "# Initialize the directory to save the model and results of task 3.a\n",
    "makedirs('./checkpoints_task3a', exist_ok=True)\n",
    "rmtree('./checkpoints_task3a')\n",
    "makedirs('./checkpoints_task3a')\n",
    "PATH = './checkpoints_task3a/policy_net'\n",
    "\n",
    "# We run the training and evaluation procedures 3 times, thus obtaining 3 different models. To analyse the\n",
    "# performance of such approach, we are going to consider the best model among the obtained ones\n",
    "\n",
    "for training_process in range(3):\n",
    "    \n",
    "    print('\\n TRAINING PROCESS :{} \\n'.format(training_process+1))\n",
    "\n",
    "    # Ensuring reproducibility, making the net initialization deterministic\n",
    "    torch.manual_seed(training_process)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    \n",
    "    # Initialize the agent for the current training_process\n",
    "    agent = DQNAgent(env, Net_Constructor=DQN, n_observations=2*9*7, n_actions=2)\n",
    "    \n",
    "    # We initialize the buffer from which we sample already observed transitions\n",
    "    # memory = ReplayMemory(20000) \n",
    "    \n",
    "    # Initialize list to keep trace of log rewards\n",
    "    training_trace = []\n",
    "    eval_trace = []\n",
    "    \n",
    "    # Looping through the episodes (500 per training process)\n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        \n",
    "        # if (i_episode % 10) == 0:\n",
    "        #     print('Training episode :{}'.format(i_episode))\n",
    "\n",
    "        # Initialize variable to keep trace of the total reward\n",
    "        total_training_reward = 0\n",
    "        \n",
    "        # Initialize the environment and get its state (moving it to the device)\n",
    "        state, info = env.reset(training_seeds[500*training_process + i_episode])\n",
    "        state = torch.tensor(state, device=device)\n",
    "\n",
    "        # We run an episode\n",
    "        for t in range(num_weeks):\n",
    "\n",
    "            action = agent.act(state, DECREASE_FLAG, EXPLORATION_FLAG, i_episode)\n",
    "            obs, reward, done, info = env.step(action.item())\n",
    "\n",
    "            total_training_reward += reward.item()\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            if done: # in our case it corresponds to 30 weeks\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "            # Store the transition in memory (in the replay buffer)\n",
    "            agent.memory.push(state, action, next_state, reward)\n",
    "            \n",
    "            # We run a training step on policy_net\n",
    "            agent.optimize_model()\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # We log the cumulative reward to training trace\n",
    "        training_trace.append(total_training_reward)\n",
    "\n",
    "\n",
    "        if ((i_episode + 1) % 5 == 0):\n",
    "\n",
    "            # We update the target network every 5 epsiodes (using a linear combination of the parameters)\n",
    "            policy_net_state_dict = agent.policy_net.state_dict()\n",
    "            target_net_state_dict = agent.target_net.state_dict()\n",
    "            \n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "\n",
    "            # Changing the parameters of the target network\n",
    "            agent.target_net.load_state_dict(target_net_state_dict)\n",
    "            \n",
    "        # If the following condition is satisfied, we compute an evaluation procedure\n",
    "        if ((i_episode + 1) % 50 == 0) or ((i_episode + 1) == 500):\n",
    "\n",
    "            # print('Evaluation cycle: {}'.format(int((i_episode + 1) / 50)))\n",
    "            average_rewards = []\n",
    "\n",
    "            for new_episode in range(num_eval_episodes):\n",
    "\n",
    "                total_eval_reward = 0\n",
    "                state, info = env.reset(eval_seeds[new_episode])\n",
    "                state = torch.tensor(state, device=device)\n",
    "\n",
    "                for new_week in range(num_weeks):\n",
    "\n",
    "                    action = agent.act(state, DECREASE_FLAG, False, new_episode) # greedy policy when eploration_flag = False\n",
    "                    obs, reward, done, info = env.step(action.item())\n",
    "                    total_eval_reward += reward.item()\n",
    "\n",
    "                    if done: # in our case it corresponds to 30 weeks\n",
    "                        next_state = None\n",
    "                    else:\n",
    "                        next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "                    # Move to the next state\n",
    "                    state = next_state\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                average_rewards.append(total_eval_reward)\n",
    "\n",
    "            eval_trace.append(np.mean(average_rewards))\n",
    "    \n",
    "    agent.save_model(PATH + str(training_process))        \n",
    "    training.append(training_trace)\n",
    "    evaluation.append(eval_trace)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01448477",
   "metadata": {},
   "source": [
    "We plot the results obtained during the training and evaluation procedures. Every figure corresponds to a different training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_eval(training, evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ee5f7f",
   "metadata": {},
   "source": [
    "We now proceed to load the best model among the 3. The choice is made depending on the last evaluation value that has been recorded for each process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0497f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select the best model after comparing the last evaluation entries for each training process\n",
    "final_performance = np.array([evaluation[0][-1], evaluation[1][-1], evaluation[2][-1]])\n",
    "best_model_idx = np.argmax(final_performance)\n",
    "\n",
    "# We initialize a new network with the parameters of the best network\n",
    "best_file = torch.load(PATH + str(best_model_idx) + '.pt')\n",
    "best_agent_const = DQNAgent(env, DQN,n_observations=2*9*7, n_actions=2)\n",
    "best_agent_const.policy_net.load_state_dict(best_file)\n",
    "\n",
    "# We create a directory to save the model. This is useful in order not to run the whole training procedure again\n",
    "makedirs('./checkpoints_final', exist_ok=True)\n",
    "rmtree('./checkpoints_final')\n",
    "makedirs('./checkpoints_final')\n",
    "PATH = './checkpoints_final/task3a'\n",
    "\n",
    "# Saving the model parameters\n",
    "torch.save(best_agent_const.policy_net.state_dict(), PATH + '.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b9c5d",
   "metadata": {},
   "source": [
    "We now proceed to simulate 3 episodes using the policy induced by the network. We then plot the results of one of these episodes to evaluate and discuss the effects of the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40468507",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Run three simulation \"\"\"\n",
    "seeds = np.arange(42, 45)\n",
    "\n",
    "log = []\n",
    "    \n",
    "agent = best_agent_const\n",
    "\n",
    "for i_episode in range(3):\n",
    "\n",
    "    episode_log = []\n",
    "    \n",
    "    # Initialize the environment and get its state (moving it to the device)\n",
    "    state, info = env.reset(seeds[i_episode])\n",
    "    episode_log.append(info)\n",
    "    state = torch.tensor(state, device=device)\n",
    "\n",
    "    # We run an episode\n",
    "    for t in range(num_weeks):\n",
    "\n",
    "        action = agent.act(state, DECREASE_FLAG, False, i_episode)\n",
    "        obs, reward, done, info = env.step(action.item())\n",
    "        episode_log.append(info)\n",
    "\n",
    "        if done: # in our case it corresponds to 30 weeks\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    log.append(episode_log)\n",
    "\n",
    "# computing useful quantities to then plot and visualize the effects of the network\n",
    "total_secondepisode = {p:np.array([getattr(l.total,p) for l in log[1]]) for p in dyn.parameters}\n",
    "cities_secondepisode = {c:{p:np.array([getattr(l.city[c],p) for l in log[1]]) for p in dyn.parameters} for c in dyn.cities}\n",
    "actions_secondepisode = {a:np.array([l.action[a] for l in log[1]]) for a in log[1][0].action.keys()}\n",
    "\n",
    "plot_info(total_secondepisode, cities_secondepisode, actions_secondepisode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d538aa8d",
   "metadata": {},
   "source": [
    "## DISCUSSIONE DA FAREEEE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b2fbbf",
   "metadata": {},
   "source": [
    "### Question 3.b: decreasing exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc82951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECREASE_FLAG indicates whether we want to decrease the exploration\n",
    "DECREASE_FLAG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b5913c",
   "metadata": {},
   "source": [
    "Now we can implement a different exploration policy by adjusting some parameters. \n",
    "\n",
    "In particular, we want to implement a decrease in the exploration threshold.\n",
    "This is done to advantage exploration over exploitation in the first stage of the training process, in order to speed up the learning process. The deeper we go in the training, the more information we have about the environment and the transitions, therefore we decrease the exploration coefficient in order to take advantage of exploitation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c47e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation parameters\n",
    "num_episodes = 500\n",
    "num_eval_episodes = 20\n",
    "num_weeks = 30\n",
    "\n",
    "# Training seeds\n",
    "training_seeds = np.random.randint(22, 10000, size=1500)\n",
    "\n",
    "# Evaluation seeds\n",
    "eval_seeds = np.arange(1, 21)\n",
    "\n",
    "# Initializing empty lists to save results\n",
    "training = []\n",
    "training_weights = []\n",
    "evaluation = []\n",
    "\n",
    "# Initialize the directory to save the model and results of task 3.b\n",
    "makedirs('./checkpoints_task3b', exist_ok=True)\n",
    "rmtree('./checkpoints_task3b')\n",
    "makedirs('./checkpoints_task3b')\n",
    "PATH = './checkpoints_task3b/policy_net'\n",
    "\n",
    "# We run the training and evaluation procedures 3 times, thus obtaining 3 different models. To analyse the\n",
    "# performance of such approach, we are going to consider the best model among the obtained ones\n",
    "for training_process in range(3):\n",
    "    \n",
    "    print('\\n TRAINING PROCESS :{} \\n'.format(training_process+1))\n",
    "    \n",
    "    torch.manual_seed(training_process)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    \n",
    "    agent = DQNAgent(env, Net_Constructor=DQN, n_observations=2*9*7, n_actions=2)\n",
    "    memory = ReplayMemory(20000) # we initialize the buffer from which we sample already observed transitions\n",
    "    \n",
    "    # Initialize list to keep trace of log rewards\n",
    "    training_trace = []\n",
    "    eval_trace = []\n",
    "    \n",
    "    # Looping through the episodes (500 per training process)\n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        \n",
    "        # if (i_episode % 10) == 0:\n",
    "        #     print('Training episode :{}'.format(i_episode))\n",
    "\n",
    "        # Initialize variable to keep trace of the total reward\n",
    "        total_training_reward = 0\n",
    "        # Initialize the environment and get its state (moving it to the device)\n",
    "        state, info = env.reset(training_seeds[500*training_process + i_episode])\n",
    "        state = torch.tensor(state, device=device)\n",
    "\n",
    "        # We run an episode\n",
    "        for t in range(num_weeks):\n",
    "\n",
    "            action = agent.act(state, DECREASE_FLAG, EXPLORATION_FLAG, i_episode)\n",
    "            obs, reward, done, info = env.step(action.item())\n",
    "\n",
    "            total_training_reward += reward.item()\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            if done: # in our case it corresponds to 30 weeks\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "            # Store the transition in memory (in the replay buffer)\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            \n",
    "            # We run a training step on policy_net\n",
    "            agent.optimize_model(memory, criterion)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # We log the cumulative reward to training trace\n",
    "        training_trace.append(total_training_reward)\n",
    "\n",
    "\n",
    "        if ((i_episode + 1) % 5 == 0):\n",
    "\n",
    "            # We update the target network every 5 epsiodes\n",
    "            policy_net_state_dict = agent.policy_net.state_dict()\n",
    "            agent.target_net.load_state_dict(policy_net_state_dict)\n",
    "\n",
    "        if ((i_episode + 1) % 50 == 0) or ((i_episode + 1) == 500):\n",
    "\n",
    "            # print('Evaluation cycle: {}'.format(int((i_episode + 1) / 50)))\n",
    "            average_rewards = []\n",
    "\n",
    "            for new_episode in range(num_eval_episodes):\n",
    "\n",
    "                total_eval_reward = 0\n",
    "                state, info = env.reset(eval_seeds[new_episode])\n",
    "                state = torch.tensor(state, device=device)\n",
    "\n",
    "                for new_week in range(num_weeks):\n",
    "\n",
    "                    action = agent.act(state, DECREASE_FLAG, False, new_episode) # greedy policy when eploration_flag = False\n",
    "                    obs, reward, done, info = env.step(action.item())\n",
    "                    total_eval_reward += reward.item()\n",
    "\n",
    "                    if done: # in our case it corresponds to 30 weeks\n",
    "                        next_state = None\n",
    "                    else:\n",
    "                        next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "                    # Move to the next state\n",
    "                    state = next_state\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                average_rewards.append(total_eval_reward)\n",
    "\n",
    "            eval_trace.append(np.mean(average_rewards))\n",
    "    \n",
    "    agent.save_model(PATH + str(training_process))        \n",
    "    training.append(training_trace)\n",
    "    training_weights.append(policy_net_state_dict)\n",
    "    evaluation.append(eval_trace)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63903022",
   "metadata": {},
   "source": [
    "We plot the obtained results to discuss the effects of the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa3c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_eval(training, evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665f3fa9",
   "metadata": {},
   "source": [
    "We now proceed to load the best model among the 3. The choice is made depending on the last evaluation value that has been recorded for each process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35842fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select the best model after comparing the last evaluation entries for each training process\n",
    "final_performance = np.array([evaluation[0][-1], evaluation[1][-1], evaluation[2][-1]])\n",
    "best_model_idx = np.argmax(final_performance)\n",
    "\n",
    "# We initialize a new network with the parameters of the best network\n",
    "best_file = torch.load(PATH + str(best_model_idx) + '.pt')\n",
    "best_agent_decreasing = DQNAgent(env, DQN,n_observations=2*9*7, n_actions=2)\n",
    "best_agent_decreasing.policy_net.load_state_dict(best_file)\n",
    "\n",
    "# We save the model. This is useful in order not to run the whole training procedure again\n",
    "PATH = './checkpoints_final/task3b'\n",
    "torch.save(best_agent_decreasing.policy_net.state_dict(), PATH + '.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7248763",
   "metadata": {},
   "source": [
    "We now proceed to simulate 3 episodes using the policy induced by the network. We then plpot the results of one of these episodes to evaluate and discuss the effects of the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e5875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Run three simulation \"\"\"\n",
    "seeds = np.arange(42, 45)\n",
    "\n",
    "log = []\n",
    "    \n",
    "agent = best_agent_decreasing\n",
    "\n",
    "for i_episode in range(3):\n",
    "\n",
    "    episode_log = []\n",
    "    \n",
    "    # Initialize the environment and get its state (moving it to the device)\n",
    "    state, info = env.reset(seeds[i_episode])\n",
    "    episode_log.append(info)\n",
    "    state = torch.tensor(state, device=device)\n",
    "\n",
    "    # We run an episode\n",
    "    for t in range(num_weeks):\n",
    "\n",
    "        action = agent.act(state, DECREASE_FLAG, False, i_episode)\n",
    "        obs, reward, done, info = env.step(action.item())\n",
    "        episode_log.append(info)\n",
    "\n",
    "        if done: # in our case it corresponds to 30 weeks\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    log.append(episode_log)\n",
    "\n",
    "total_secondepisode = {p:np.array([getattr(l.total,p) for l in log[1]]) for p in dyn.parameters}\n",
    "cities_secondepisode = {c:{p:np.array([getattr(l.city[c],p) for l in log[1]]) for p in dyn.parameters} for c in dyn.cities}\n",
    "actions_secondepisode = {a:np.array([l.action[a] for l in log[1]]) for a in log[1][0].action.keys()}\n",
    "\n",
    "plot_info(total_secondepisode, cities_secondepisode, actions_secondepisode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc35e55",
   "metadata": {},
   "source": [
    "### Question 3.c: Evaluate the best performing policy against Pr. Russoâ€™s policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e5889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Russo Agent\n",
    "agent = best_agent_decreasing\n",
    "# Initializing the seeds for reproducibility purposes\n",
    "seeds = range(1,51)\n",
    "\n",
    "# Initializing useful variables to store results\n",
    "rewards = []\n",
    "deaths = []\n",
    "\n",
    "# Initializing confinement weeks count variable\n",
    "confinement_weeks_count = 0\n",
    "\n",
    "# Looping over 50 episodes\n",
    "for i_episode in range(3):\n",
    "\n",
    "    episode_log = []\n",
    "    \n",
    "    # Initialize the environment and get its state (moving it to the device)\n",
    "    state, info = env.reset(seeds[i_episode])\n",
    "    episode_log.append(info)\n",
    "    state = torch.tensor(state, device=device)\n",
    "\n",
    "    # We run an episode\n",
    "    for t in range(num_weeks):\n",
    "        \n",
    "        past_action = dyn.get_action()\n",
    "        if past_action['confinement'] == True:\n",
    "            confinement_weeks_count+= 1\n",
    "\n",
    "        action = agent.act(state, DECREASE_FLAG, False, i_episode)\n",
    "        obs, reward, done, info = env.step(action.item())\n",
    "        episode_log.append(info)\n",
    "\n",
    "        if done: # in our case it corresponds to 30 weeks\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    log.append(episode_log)\n",
    "    \n",
    "    \"\"\" Parse the logs \"\"\"\n",
    "    # Saving total number of confined days\n",
    "    conf_days.append(7 * confinement_weeks_count)\n",
    "    # R_cumulative is computed in the inner loop\n",
    "    rewards.append(R_cumulative)\n",
    "    # Number of total deaths in the current episode\n",
    "    deaths.append(info.total.dead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41531c8",
   "metadata": {},
   "source": [
    "We plot the histograms of the required quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b9cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms(deaths, rewards, conf_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f03cb1",
   "metadata": {},
   "source": [
    "# TO DO: DISCUSSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b696909",
   "metadata": {},
   "source": [
    "## Question 4: Dealing with a more complex action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b8e5ea",
   "metadata": {},
   "source": [
    "In order to answer the questions of this task, we are asked to use 10e-5 as a learning rate. Therefore, we set this new value in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e103d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate\n",
    "LR = 10e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc3a7a",
   "metadata": {},
   "source": [
    "Moreover, despite the changes we need to make for what regards the action and observation spaces, the structure (architecture) of the agent is unchanged for now. Therefore, in this first part, we will use the above defined class `DQN`, this time initialized using `n_action` = 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3615e23f",
   "metadata": {},
   "source": [
    "## Question 4.1: Action space design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeb337c",
   "metadata": {},
   "source": [
    "### Question 4.1.a: (Theory) Action space design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf05a88",
   "metadata": {},
   "source": [
    "con 5 neuroni in piÃ¹ nell'input (embedding dell'azione alla settimana prima) ci fa risparmiare 2^5 neuroni in uscita\n",
    "in uscita abbiamo quindi 5 neuroni invece che 2^5\n",
    "\n",
    "nel caso tabular con (stato agnostico alla situazione attuale sulle azioni), in output dovremmo quindi avere 2^5 neuroni, il che vorrebbe dire che potenzialmente potremmo prendere piÃ¹ 'toggle' contemporaneamente, che non Ã¨ possibile\n",
    "\n",
    "dando informazioni sull'azione in input, invece, possiamo ridurre l'output a 5 neuroni e poi applicare una softmax per scegliere quale toggle applicare\n",
    "In questo modo imponiamo che possa essere switchata solo una azione alla volta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c136e43",
   "metadata": {},
   "source": [
    "### Question 4.1.b: Toggle-action-space multi-action policy training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5346968",
   "metadata": {},
   "source": [
    "After discussing how the new choice of action and observation space affects the network architecture, we proceed to implement the toggled-action and observation spaces to train the Deep Q-Learning agent.\n",
    "\n",
    "For what regards the action space, since we are only allowed to toggle, we can still consider the action to be a scalar indicating for which action we need to change the state (from True to False, from activated to deactivated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Preprocessor: the deafult action (Do Nothing) is encoded as 0\n",
    "# All the remaining actions are encoded as a single number\n",
    "\n",
    "TOGGLE_NULL = 0\n",
    "TOGGLE_CONFINEMENT = 1\n",
    "TOGGLE_ISOLATION = 2\n",
    "TOGGLE_HOSPITAL = 3\n",
    "TOGGLE_VACCINATION = 4\n",
    "\n",
    "def action_preprocessor(a:torch.Tensor, dyn:ModelDynamics):\n",
    "    \n",
    "    # As described above, the input argument a corresponds to a scalar, indicating which \n",
    "    # action has to be toggled\n",
    "    \n",
    "    default_action = { # DO NOTHING\n",
    "        'confinement': False, \n",
    "        'isolation': False, \n",
    "        'hospital': False, \n",
    "        'vaccinate': False}\n",
    "    \n",
    "    if a not in range(0, 5):\n",
    "        print('Not a valid action!')\n",
    "        return default_action\n",
    "    \n",
    "    \n",
    "    # We get the state of the actions used in the past week. Starting from this configuration,\n",
    "    # we toggle the action according to the information carried by the input argument a\n",
    "    action = dyn.get_action()\n",
    "    \n",
    "    if a == TOGGLE_NULL: \n",
    "        return action\n",
    "    elif a == TOGGLE_CONFINEMENT:\n",
    "        action['confinement'] = not action['confinement'] \n",
    "    elif a == TOGGLE_ISOLATION:\n",
    "        action['isolation'] = not action['isolation']\n",
    "    elif a == TOGGLE_HOSPITAL:\n",
    "        action['hospital'] = not action['hospital']\n",
    "    else: \n",
    "        action['vaccinate'] = not action['vaccinate']\n",
    "    \n",
    "    return action\n",
    "\n",
    "# Observation Preprocessor: every observation is converted to a tensor containing the proportion of death and\n",
    "# infected people in each city. Since we need to consider also the actions' states, we eventually return a \n",
    "# tensor of shape (1,130) as a result of the flattining operation and the concatenation\n",
    "\n",
    "SCALE = 1\n",
    "def observation_preprocessor(obs: Observation, dyn:ModelDynamics):\n",
    "    infected = SCALE * np.array([np.array(obs.city[c].infected)/obs.pop[c] for c in dyn.cities])\n",
    "    dead = SCALE * np.array([np.array(obs.city[c].dead)/obs.pop[c] for c in dyn.cities])\n",
    "    \n",
    "    curr_action = torch.Tensor(4)\n",
    "    curr_action[0] = 1 if dyn.get_action()['confinement'] else 0\n",
    "    curr_action[1] = 1 if dyn.get_action()['isolation'] else 0\n",
    "    curr_action[2] = 1 if dyn.get_action()['hospital'] else 0\n",
    "    curr_action[3] = 1 if dyn.get_action()['vaccinate'] else 0\n",
    "    \n",
    "    ret = torch.flatten(torch.Tensor(np.stack((infected, dead))).unsqueeze(0),1)\n",
    "    ret = torch.cat([ret, curr_action.unsqueeze(dim=0)], dim=1) # shape (1,130)\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac93edcb",
   "metadata": {},
   "source": [
    "Since we have defined a new action and observation spaces, as well as new preprocessors, we need to redefine the environment we are going to interact with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the environment\n",
    "env = Env(  dyn, # We pass the dynamical model to the environment \n",
    "            action_space=spaces.Discrete(5) , # Since we can toggle 1 out of 4 actions, or DO NOTHING\n",
    "            # observation_space=spaces.Box(low=0, high=1, shape=(2,9,7)),\n",
    "            action_preprocessor=action_preprocessor,\n",
    "            observation_preprocessor=observation_preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ded212",
   "metadata": {},
   "source": [
    "We now compute three training processes, in order to then plot the average training and evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 500\n",
    "num_eval_episodes = 20\n",
    "num_weeks = 30\n",
    "\n",
    "# training seeds\n",
    "training_seeds = np.random.randint(22, 10000, size=1500)\n",
    "\n",
    "# evaluation seeds\n",
    "eval_seeds = np.arange(1, 21)\n",
    "\n",
    "training = []\n",
    "training_weights = []\n",
    "evaluation = []\n",
    "\n",
    "makedirs('./checkpoints_task4a', exist_ok=True)\n",
    "rmtree('./checkpoints_task4a')\n",
    "makedirs('./checkpoints_task4a')\n",
    "PATH = './checkpoints_task4a/policy_net'\n",
    "\n",
    "n_observations = 2*9*7 + 4\n",
    "n_actions = 5\n",
    "\n",
    "for training_process in range(3):\n",
    "    \n",
    "    print('\\n TRAINING PROCESS :{} \\n'.format(training_process+1))\n",
    "    \n",
    "    torch.manual_seed(training_process)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    \n",
    "    agent = DQNAgent(env, Net_Constructor=DQN, n_observations= n_observations, n_actions=n_actions)\n",
    "    memory = ReplayMemory(20000) # we initialize the buffer from which we sample already observed transitions\n",
    "    \n",
    "    # Initialize list to keep trace of log rewards\n",
    "    training_trace = []\n",
    "    eval_trace = []\n",
    "\n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        \n",
    "        # if (i_episode % 10) == 0:\n",
    "        #     print('Training episode :{}'.format(i_episode))\n",
    "\n",
    "        # Initialize variable to keep trace of the total reward\n",
    "        total_training_reward = 0\n",
    "        # Initialize the environment and get its state (moving it to the device)\n",
    "        state, info = env.reset(training_seeds[500*training_process + i_episode])\n",
    "        state = torch.tensor(state, device=device)\n",
    "\n",
    "        # We run an episode\n",
    "        for t in range(num_weeks):\n",
    "\n",
    "            action = agent.act(state, DECREASE_FLAG, EXPLORATION_FLAG, i_episode)\n",
    "            obs, reward, done, info = env.step(action.item())\n",
    "\n",
    "            total_training_reward += reward.item()\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            if done: # in our case it corresponds to 30 weeks\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "            # Store the transition in memory (in the replay buffer)\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            \n",
    "            # We run a training step on policy_net\n",
    "            agent.optimize_model(memory, criterion)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # We log the cumulative reward to training trace\n",
    "        training_trace.append(total_training_reward)\n",
    "\n",
    "        if ((i_episode + 1) % 5 == 0):\n",
    "\n",
    "            # We update the target network every 5 epsiodes\n",
    "            policy_net_state_dict = agent.policy_net.state_dict()\n",
    "            agent.target_net.load_state_dict(policy_net_state_dict)\n",
    "\n",
    "        if ((i_episode + 1) % 50 == 0) or ((i_episode + 1) == 500):\n",
    "\n",
    "            # print('Evaluation cycle: {}'.format(int((i_episode + 1) / 50)))\n",
    "            average_rewards = []\n",
    "\n",
    "            for new_episode in range(num_eval_episodes):\n",
    "\n",
    "                total_eval_reward = 0\n",
    "                state, info = env.reset(eval_seeds[new_episode])\n",
    "                state = torch.tensor(state, device=device)\n",
    "\n",
    "                for new_week in range(num_weeks):\n",
    "\n",
    "                    action = agent.act(state, DECREASE_FLAG, False, new_episode) # greedy policy when eploration_flag = False\n",
    "                    obs, reward, done, info = env.step(action.item())\n",
    "                    total_eval_reward += reward.item()\n",
    "\n",
    "                    if done: # in our case it corresponds to 30 weeks\n",
    "                        next_state = None\n",
    "                    else:\n",
    "                        next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "                    # Move to the next state\n",
    "                    state = next_state\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                average_rewards.append(total_eval_reward)\n",
    "\n",
    "            eval_trace.append(np.mean(average_rewards))\n",
    "    \n",
    "    agent.save_model(PATH + str(training_process))        \n",
    "    training.append(training_trace)\n",
    "    training_weights.append(policy_net_state_dict)\n",
    "    evaluation.append(eval_trace)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c26940d",
   "metadata": {},
   "source": [
    "After computing and logging the results, we plot and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d4101",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_eval(training, evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88d32fb",
   "metadata": {},
   "source": [
    "After choosing the best model, we run 3 episodes and plot the results of one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221fd626",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_performance = np.array([evaluation[0][-1], evaluation[1][-1], evaluation[2][-1]])\n",
    "best_model_idx = np.argmax(final_performance)\n",
    "\n",
    "best_file = torch.load(PATH + str(best_model_idx) + '.pt')\n",
    "best_agent = DQNAgent(env, DQN,n_observations=n_observations, n_actions=n_actions)\n",
    "best_agent.policy_net.load_state_dict(best_file)\n",
    "\n",
    "PATH = './checkpoints_final/task4a'\n",
    "\n",
    "torch.save(best_agent_decreasing.policy_net.state_dict(), PATH + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f288b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Run three simulation \"\"\"\n",
    "seeds = np.arange(42, 45)\n",
    "\n",
    "log = []\n",
    "    \n",
    "agent = best_agent\n",
    "\n",
    "for i_episode in range(3):\n",
    "\n",
    "    episode_log = []\n",
    "    \n",
    "    # Initialize the environment and get its state (moving it to the device)\n",
    "    state, info = env.reset(seeds[i_episode])\n",
    "    episode_log.append(info)\n",
    "    state = torch.tensor(state, device=device)\n",
    "\n",
    "    # We run an episode\n",
    "    for t in range(num_weeks):\n",
    "\n",
    "        action = agent.act(state, DECREASE_FLAG, False, i_episode)\n",
    "        obs, reward, done, info = env.step(action.item())\n",
    "        episode_log.append(info)\n",
    "\n",
    "        if done: # in our case it corresponds to 30 weeks\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    log.append(episode_log)\n",
    "\n",
    "total_secondepisode = {p:np.array([getattr(l.total,p) for l in log[1]]) for p in dyn.parameters}\n",
    "cities_secondepisode = {c:{p:np.array([getattr(l.city[c],p) for l in log[1]]) for p in dyn.parameters} for c in dyn.cities}\n",
    "actions_secondepisode = {a:np.array([l.action[a] for l in log[1]]) for a in log[1][0].action.keys()}\n",
    "\n",
    "plot_info(total_secondepisode, cities_secondepisode, actions_secondepisode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9de2876",
   "metadata": {},
   "source": [
    "### Question 4.1.c: Toggle-action-space multi-action policy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe043db",
   "metadata": {},
   "source": [
    "As we previously did for Russo's policy, we proceed to plot 50 episodes to then visualize the results using the plot histograms function provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2239a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Russo Agent\n",
    "agent = best_agent\n",
    "# Initializing the seeds for reproducibility purposes\n",
    "seeds = range(1,51)\n",
    "\n",
    "# Initializing useful variables to store results\n",
    "rewards = []\n",
    "deaths = []\n",
    "\n",
    "# Initializing confinement weeks count variable\n",
    "confinement_weeks_count = 0\n",
    "\n",
    "# Looping over 50 episodes\n",
    "for i_episode in range(50):\n",
    "\n",
    "    episode_log = []\n",
    "    \n",
    "    # Initialize the environment and get its state (moving it to the device)\n",
    "    state, info = env.reset(seeds[i_episode])\n",
    "    episode_log.append(info)\n",
    "    state = torch.tensor(state, device=device)\n",
    "\n",
    "    # We run an episode\n",
    "    for t in range(num_weeks):\n",
    "        \n",
    "        past_action = dyn.get_action()\n",
    "        if past_action['confinement'] == True:\n",
    "            confinement_weeks_count += 1\n",
    "\n",
    "        action = agent.act(state, DECREASE_FLAG, False, i_episode)\n",
    "        obs, reward, done, info = env.step(action.item())\n",
    "        episode_log.append(info)\n",
    "\n",
    "        if done: # in our case it corresponds to 30 weeks\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    log.append(episode_log)\n",
    "    \n",
    "    \"\"\" Parse the logs \"\"\"\n",
    "    # Saving total number of confined days\n",
    "    conf_days.append(7 * confinement_weeks_count)\n",
    "    # R_cumulative is computed in the inner loop\n",
    "    rewards.append(R_cumulative)\n",
    "    # Number of total deaths in the current episode\n",
    "    deaths.append(info.total.dead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88335b9b",
   "metadata": {},
   "source": [
    "We plot the histograms of the required quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74c1f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms(deaths, rewards, conf_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25627ac0",
   "metadata": {},
   "source": [
    "### Question 4.1.d: (Theory) question about toggled-action-space policy, what assumption does it make?\n",
    "\n",
    "\n",
    "la distanza (in termini di metrica) fra un'azione e la successiva deve essere al piÃ¹ 1\n",
    "ossia possiamo fare il toggling solo di una azione alla volta (al massimo... volendo si puÃ² anche non cambiare nulla)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7949cdbf",
   "metadata": {},
   "source": [
    "## 4.2 Factorized Q-values, multi-action agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c046ccf3",
   "metadata": {},
   "source": [
    "### Question 4.2.a: Multi-action factorized Q-values policy training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba2d91b",
   "metadata": {},
   "source": [
    "Instead of implementing a toggled behavior as above you let the neural network decide whether to make each decision True or False independently. For this purpose, we need to change the action preprocessor and the observation preprocessor, as well as the spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b68536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_preprocessor(a:torch.Tensor, dyn:ModelDynamics):\n",
    "    \n",
    "    # a is a pytorch tensor of shape (1,4)\n",
    "    \n",
    "    action = { # DO NOTHING\n",
    "        'confinement': False, \n",
    "        'isolation': False, \n",
    "        'hospital': False, \n",
    "        'vaccinate': False}\n",
    "    \n",
    "    action['confinement'] = bool(a[0][0])\n",
    "    action['isolation'] = bool(a[0][1])\n",
    "    action['hospital'] = bool(a[0][2])\n",
    "    action['vaccinate'] = bool(a[0][3])\n",
    "    \n",
    "    return action\n",
    "\n",
    "# Observation Preprocessor: every observation is converted to a tensor containing the proportion of death and\n",
    "# infected people in each city\n",
    "\n",
    "SCALE = 1\n",
    "def observation_preprocessor(obs: Observation, dyn:ModelDynamics):\n",
    "    infected = SCALE * np.array([np.array(obs.city[c].infected)/obs.pop[c] for c in dyn.cities])\n",
    "    dead = SCALE * np.array([np.array(obs.city[c].dead)/obs.pop[c] for c in dyn.cities])\n",
    "    return torch.Tensor(np.stack((infected, dead))).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb529bf6",
   "metadata": {},
   "source": [
    "We now initialize a new environment, giving the new preprocessors defined above as input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c2d98",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initializing the environment\n",
    "env = Env(  dyn, # We pass the dynamical model to the environment \n",
    "            action_space=spaces.MultiBinary([1,4]) , # Here one could pass an openai gym action space that can then be sampled\n",
    "            observation_space=spaces.Box(low=0, high=1, shape=(2,9,7)),\n",
    "            action_preprocessor=action_preprocessor,\n",
    "            observation_preprocessor=observation_preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc8941",
   "metadata": {},
   "source": [
    "Because we now allow the agent to take 16 actions (True or False for every one of the four actions), we also need to change the architecture of the neural network we use as an agent. We put particular attention on the output layer, which now returns a tensor of shape (4,2). Every row now represents and action, and every column indicates whether we take (col_id = 1) or not (col_id = 0) that specific action. In order to define the action to take after computing the output value, we need to compute, for each row of the output tensor, the argmax between the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f87b25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_task4(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        # MLP network\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_observations, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 16),\n",
    "            torch.nn.ReLU(),  \n",
    "            torch.nn.Linear(16, n_actions) # here n agent is set to 8\n",
    "        )\n",
    "        \n",
    "    # Called in order to compute the forward pass in the network\n",
    "    def forward(self, x):\n",
    "        new_x = torch.flatten(x,1) ** (1/4) # since we might end up having very small input values due to the scaling\n",
    "                                            # We flatten the input in order to use it in the linear network\n",
    "        z = self.mlp(new_x)\n",
    "        \n",
    "        return torch.reshape(z, (-1,4,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67683cac",
   "metadata": {},
   "source": [
    "We now redefine the agent class, using the above defined constructor in order to define and initialize both the policy and the target networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fb4cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgentTotal(Agent): #superclass\n",
    "    def __init__(self, env: Env, Net_Constructor, n_observations, n_actions):\n",
    "        \"\"\"\n",
    "        Example agent implementation. Just picks a random action at each time step.\n",
    "        \"\"\"\n",
    "        # MLP networks. Policy net and target net are used in order to compute the Qvalues\n",
    "        self.policy_net = Net_Constructor(n_observations, n_actions).to(device)\n",
    "        self.target_net = Net_Constructor(n_observations, n_actions).to(device)\n",
    "        # We initialize the target network using the parameters of the policy network\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        # We initialize the optimizer using the suggeseted learning rate (it must act on policy_net parameters)\n",
    "        self.optimizer = torch.optim.AdamW(self.policy_net.parameters(), lr=LR, amsgrad=True) # we initialize the optimizer\n",
    "        self.env = env\n",
    "\n",
    "    def load_model(self, PATH):\n",
    "        \"\"\"Routine for loading a pre-trained model\"\"\"\n",
    "        self.policy_net.load_state_dict(PATH)\n",
    "        pass\n",
    "\n",
    "    def save_model(self, PATH):\n",
    "        \"\"\"Routine for saving the weights for a trained model\"\"\"\n",
    "        torch.save(self.policy_net.state_dict(), PATH + '.pt')\n",
    "\n",
    "    def optimize_model(self, memory, criterion):\n",
    "        \n",
    "        \"\"\"Routine to perform the optimization step updating policy net parameters\"\"\"\n",
    "        \n",
    "        # If there aren't enough data in the memory buffer, avoid computing update step\n",
    "        if len(memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        # Sampling the transitions from the memory buffer\n",
    "        transitions = memory.sample(BATCH_SIZE)\n",
    "        # Transpose the batch. This converts batch-array of Transitions to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements, moving the final results to the device\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "        \n",
    "        # We save the non final state in our sampling\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        \n",
    "        # We save and concatenate the variables we need\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.policy_net(state_batch)\n",
    "        state_action_values = torch.sum(state_action_values.gather(2, action_batch.unsqueeze(2)), dim=1)\n",
    "        \n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            # we compute max_a Q(s', a) in order to quantify the delta. We use the target net to improve stability\n",
    "            next_state_values[non_final_mask] = torch.sum(self.target_net(non_final_next_states).max(2)[0], dim=1)\n",
    "            \n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def reset(self,):\n",
    "        # This should be called when the environment is reset\n",
    "        pass\n",
    "        \n",
    "    def act(self, state, decrease_flag, exploration_flag, curr_episode):\n",
    "        \"\"\" The following function defines the policy deployed in order to explore and interact with the environments.\n",
    "            In particular, it is an epsilon greedy policy (if exploration_flag = True) based on the values provided by \n",
    "            the network trained up to this moment \"\"\"\n",
    "        \n",
    "        # The method takes an observation and returns an action\n",
    "        # In case of random action, the action can be directly sampled using self.env.action_space.sample()\n",
    "        sample = random.random()\n",
    "\n",
    "        # In case of decreasing exploration rate, the following variable is needed\n",
    "        num_episodes = 500\n",
    "        \n",
    "        # Setting the epsilon parameter to have or avoid exploration\n",
    "        if exploration_flag:\n",
    "            if decrease_flag:\n",
    "                eps_threshold = max(EPS_START*(num_episodes-curr_episode)/num_episodes, EPS_MIN)\n",
    "            else:\n",
    "                eps_threshold = EPS_START\n",
    "        else:\n",
    "            eps_threshold = 0 # in this case we only consider the greedy policy determined by the network\n",
    "            \n",
    "        if sample >= eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                # We use torch.no_grad() since we are now using the network to simply take an action.\n",
    "                # We do not need to expand the DAG and consider this computation in the backprop step.\n",
    "                # We only consider the larger value to take the action.\n",
    "                return self.policy_net(state).max(2)[1]\n",
    "        else:\n",
    "            return torch.tensor(agent.env.action_space.sample(), device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bd574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 500\n",
    "num_eval_episodes = 20\n",
    "num_weeks = 30\n",
    "\n",
    "# training seeds\n",
    "training_seeds = np.random.randint(22, 10000, size=1500)\n",
    "\n",
    "# evaluation seeds\n",
    "eval_seeds = np.arange(1, 21)\n",
    "\n",
    "training = []\n",
    "training_weights = []\n",
    "evaluation = []\n",
    "\n",
    "makedirs('./checkpoints_task4_2', exist_ok=True)\n",
    "rmtree('./checkpoints_task4_2')\n",
    "makedirs('./checkpoints_task4_2')\n",
    "PATH = './checkpoints_task4_2/policy_net'\n",
    "\n",
    "n_observations = 2*9*7\n",
    "n_actions = 8\n",
    "\n",
    "for training_process in range(3):\n",
    "    \n",
    "    print('\\n TRAINING PROCESS :{} \\n'.format(training_process+1))\n",
    "    \n",
    "    torch.manual_seed(training_process)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    \n",
    "    agent = DQNAgentTotal(env, Net_Constructor=DQN_task4, n_observations= n_observations, n_actions=n_actions)\n",
    "    memory = ReplayMemory(20000) # we initialize the buffer from which we sample already observed transitions\n",
    "    \n",
    "    # Initialize list to keep trace of log rewards\n",
    "    training_trace = []\n",
    "    eval_trace = []\n",
    "\n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        \n",
    "        # if (i_episode % 10) == 0:\n",
    "        #     print('Training episode :{}'.format(i_episode))\n",
    "\n",
    "        # Initialize variable to keep trace of the total reward\n",
    "        total_training_reward = 0\n",
    "        # Initialize the environment and get its state (moving it to the device)\n",
    "        state, info = env.reset(training_seeds[500*training_process + i_episode])\n",
    "        state = torch.tensor(state, device=device)\n",
    "\n",
    "        # We run an episode\n",
    "        for t in range(num_weeks):\n",
    "\n",
    "            action = agent.act(state, DECREASE_FLAG, EXPLORATION_FLAG, i_episode) # tensor of shape (1, 1, 4)\n",
    "            \n",
    "            obs, reward, done, info = env.step(action)\n",
    "\n",
    "            total_training_reward += reward.item()\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            if done: # in our case it corresponds to 30 weeks\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "            # Store the transition in memory (in the replay buffer)\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            \n",
    "            # We run a training step on policy_net\n",
    "            agent.optimize_model(memory, criterion)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # We log the cumulative reward to training trace\n",
    "        training_trace.append(total_training_reward)\n",
    "\n",
    "        if ((i_episode + 1) % 5 == 0):\n",
    "\n",
    "            # We update the target network every 5 epsiodes\n",
    "            policy_net_state_dict = agent.policy_net.state_dict()\n",
    "            agent.target_net.load_state_dict(policy_net_state_dict)\n",
    "\n",
    "        if ((i_episode + 1) % 50 == 0) or ((i_episode + 1) == 500):\n",
    "\n",
    "            # print('Evaluation cycle: {}'.format(int((i_episode + 1) / 50)))\n",
    "            average_rewards = []\n",
    "\n",
    "            for new_episode in range(num_eval_episodes):\n",
    "\n",
    "                total_eval_reward = 0\n",
    "                state, info = env.reset(eval_seeds[new_episode])\n",
    "                state = torch.tensor(state, device=device)\n",
    "\n",
    "                for new_week in range(num_weeks):\n",
    "\n",
    "                    action = agent.act(state, DECREASE_FLAG, False, new_episode) # greedy policy when eploration_flag = False\n",
    "                    obs, reward, done, info = env.step(action)\n",
    "                    total_eval_reward += reward.item()\n",
    "\n",
    "                    if done: # in our case it corresponds to 30 weeks\n",
    "                        next_state = None\n",
    "                    else:\n",
    "                        next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "                    # Move to the next state\n",
    "                    state = next_state\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                average_rewards.append(total_eval_reward)\n",
    "\n",
    "            eval_trace.append(np.mean(average_rewards))\n",
    "    \n",
    "    agent.save_model(PATH + str(training_process))        \n",
    "    training.append(training_trace)\n",
    "    training_weights.append(policy_net_state_dict)\n",
    "    evaluation.append(eval_trace)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c2387",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_performance = np.array([evaluation[0][-1], evaluation[1][-1], evaluation[2][-1]])\n",
    "best_model_idx = np.argmax(final_performance)\n",
    "\n",
    "best_file = torch.load(PATH + str(best_model_idx) + '.pt')\n",
    "best_agent = DQNAgentTotal(env, DQN_task4, n_observations=n_observations, n_actions=n_actions)\n",
    "best_agent.policy_net.load_state_dict(best_file)\n",
    "\n",
    "PATH = './checkpoints_final/task4_2'\n",
    "\n",
    "torch.save(best_agent_decreasing.policy_net.state_dict(), PATH + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d66b6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Russo Agent\n",
    "agent = best_agent\n",
    "# Initializing the seeds for reproducibility purposes\n",
    "seeds = range(1,51)\n",
    "\n",
    "# Initializing useful variables to store results\n",
    "rewards = []\n",
    "deaths = []\n",
    "\n",
    "# Initializing confinement weeks count variable\n",
    "confinement_weeks_count = 0\n",
    "\n",
    "# Looping over 50 episodes\n",
    "for i_episode in range(50):\n",
    "\n",
    "    episode_log = []\n",
    "    \n",
    "    # Initialize the environment and get its state (moving it to the device)\n",
    "    state, info = env.reset(seeds[i_episode])\n",
    "    episode_log.append(info)\n",
    "    state = torch.tensor(state, device=device)\n",
    "\n",
    "    # We run an episode\n",
    "    for t in range(num_weeks):\n",
    "        \n",
    "        past_action = dyn.get_action()\n",
    "        if past_action['confinement'] == True:\n",
    "            confinement_weeks_count += 1\n",
    "\n",
    "        action = agent.act(state, DECREASE_FLAG, False, i_episode)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_log.append(info)\n",
    "\n",
    "        if done: # in our case it corresponds to 30 weeks\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, device=device)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    log.append(episode_log)\n",
    "    \n",
    "    \"\"\" Parse the logs \"\"\"\n",
    "    # Saving total number of confined days\n",
    "    conf_days.append(7 * confinement_weeks_count)\n",
    "    # R_cumulative is computed in the inner loop\n",
    "    rewards.append(R_cumulative)\n",
    "    # Number of total deaths in the current episode\n",
    "    deaths.append(info.total.dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ad19d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms(deaths, rewards, conf_days)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
